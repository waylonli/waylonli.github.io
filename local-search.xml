<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Singular Value Decomposition</title>
    <link href="/2020/10/27/SVD/"/>
    <url>/2020/10/27/SVD/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to Singular Value Decomposition (SVD)</p><a id="more"></a><h1 id="1-What-is-SVD-Singular-Value-Decomposition"><a href="#1-What-is-SVD-Singular-Value-Decomposition" class="headerlink" title="1. What is SVD (Singular Value Decomposition)?"></a>1. What is SVD (Singular Value Decomposition)?</h1><h2 id="1-1-Definition"><a href="#1-1-Definition" class="headerlink" title="1.1 Definition"></a>1.1 Definition</h2><p>Define $ \mathbf{ X } = (\vec{x_1}) $  as a $n\times m$ matrix ($n \ge m$). A fraction of $\mathbf{X}$ is:<br>$$<br>\mathbf{X} = \mathbf{ U }\mathbf{ \Sigma } \mathbf{ V }^\top<br>$$<br>where:</p><ul><li><p>$ \mathbf{U} $: $n \times n$ unitary matrix ($\mathbf{U}^* = \mathbf{U}^{-1}$)</p></li><li><p>$ \mathbf{\Sigma} $: $n \times m$ rectangular diagonal matrix with non-negative real numbers on the diagonal</p><p>$ \mathbf{\Sigma} = \begin{bmatrix} \sigma_1 \\ &amp; \sigma_2 \\ &amp; &amp; \ddots \\ &amp; &amp; &amp; \sigma_m \\ 0 &amp; 0 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; 0 \end{bmatrix} $, $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_m \ge 0$ </p></li></ul><ul><li>$ \mathbf{V} $: $m\times m$ unitary matrix</li></ul><h2 id="1-2-Singular-value-and-singular-vector"><a href="#1-2-Singular-value-and-singular-vector" class="headerlink" title="1.2 Singular value and singular vector"></a>1.2 Singular value and singular vector</h2><p><strong>Singular value:</strong> the value on the diagonal in matrix $\mathbf{\Sigma}$ </p><p><strong>Singular vector:</strong></p><ul><li>Left singular vector: $ \mathbf{U} = (\vec{u_1}, \vec{u_2}, \dots , \vec{u_n}) $  are the left singular vectors of matrix $\mathbf{X}$ </li><li>Right singular vector: $ \mathbf{V} = (\vec{v_1}, \vec{v_2}, \dots , \vec{v_m}) $ are the right singular vectors of matrix $\mathbf{X}$ </li></ul><h1 id="2-Matrix-Approximation"><a href="#2-Matrix-Approximation" class="headerlink" title="2. Matrix Approximation"></a>2. Matrix Approximation</h1><p>Notice that  there are only $m$ singular values, we have the Eckand-Young theorem (1936):<br>$$<br>\mathbf{X} = \sigma_1 u_1 v_1^\top + \sigma_2 u_2 v_2^\top + \dots + \sigma_m u_m v_m^\top + \mathbf{0} \approx \mathbf{\tilde{U}}\mathbf{\tilde{\Sigma}}\mathbf{\tilde{V}}^\top<br>$$</p><h1 id="3-Dominant-Correlations"><a href="#3-Dominant-Correlations" class="headerlink" title="3. Dominant Correlations"></a>3. Dominant Correlations</h1><img src="https://waylonli.com/img/mdimg/image-20201027133303454.png" alt="image-20201027133303454" style="zoom:50%;" align=center /><h1 id="4-SVD-and-Truncation"><a href="#4-SVD-and-Truncation" class="headerlink" title="4. SVD and Truncation"></a>4. SVD and Truncation</h1><p>In $\mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top$, we can choose $r$ columns of $\mathbf{U}$ and $\mathbf{V}$. “How to choose $r$” is a fundamental problem in SVD.<br>$$<br>\mathbf{X} = \mathbf{\tilde{U}}_r\mathbf{\tilde{\Sigma}}_r\mathbf{\tilde{V}}_r^\top<br>$$<br>Generally we need to set a threshold for $\log \sigma$ and choose those $\sigma$s beyond the threshold on the diagonal of $\mathbf{\Sigma}$.</p><p><strong>Recommending extra reading:</strong> <a href="https://arxiv.org/pdf/1305.5870.pdf">Gavish-Donoho (2014)</a></p><p>Applying a Gaussian noise to $\mathbf{X}$:</p><p>$$<br>\mathbf{X} = \mathbf{X}_\text{true} + \delta \mathbf{X}_\text{noise}<br>$$</p><p>PS: I did not mange to fully understand the idea here, more details about the truncation can be found in the video and the paper using the URL in this section</p><h1 id="5-Example-Image-Compression"><a href="#5-Example-Image-Compression" class="headerlink" title="5. Example: Image Compression"></a>5. Example: Image Compression</h1><p>To better understand what SVD does in a practical task, we consider a image processing task. It is easy to represent an image using a matrix. After calculate the SVD of this matrix, by choosing different $r$, we can keep different features of the image. When we choose to keep all the singular values, we can get the original image.</p><img src="https://waylonli.com/img/mdimg/image-20201026234621629.png" alt="image-20201026234621629" style="zoom:100%;" /><p><strong>Videos</strong>:</p><ul><li><a href="https://www.youtube.com/watch?v=gXbThCXjZFM">Overview</a></li><li><a href="https://www.youtube.com/watch?v=nbBvuuNVfco">Mathematical Overview</a></li><li><a href="https://www.youtube.com/watch?v=xy3QyyhiuY4">Matrix Approximation</a></li><li><a href="https://www.youtube.com/watch?v=WmDnaoY2Ivs">Dominant Correlations</a></li><li><a href="https://www.youtube.com/watch?v=QQ8vxj-9OfQ&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv&index=5">Image Compression</a></li><li><a href="https://www.youtube.com/watch?v=9vJDjkx825k&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv&index=34">Optimal Truncation</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SVD</tag>
      
      <tag>Dimension Reduction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>16-Semantic Role Labelling and Argument Structure</title>
    <link href="/2020/10/14/16-Semantic%20Role%20Labelling%20and%20Argument%20Structure/"/>
    <url>/2020/10/14/16-Semantic%20Role%20Labelling%20and%20Argument%20Structure/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Semantic-Roles"><a href="#1-Semantic-Roles" class="headerlink" title="1. Semantic Roles"></a>1. Semantic Roles</h1><h2 id="1-1-Commonly-used-thematic-roles"><a href="#1-1-Commonly-used-thematic-roles" class="headerlink" title="1.1 Commonly used thematic roles"></a>1.1 Commonly used thematic roles</h2><img src="https://waylonli.com/img/mdimg/image-20200318211033606.png" alt="image-20200318211033606" style="zoom:50%;" /><h2 id="1-2-Issues-with-thematic-roles"><a href="#1-2-Issues-with-thematic-roles" class="headerlink" title="1.2 Issues with thematic roles"></a>1.2 Issues with thematic roles</h2><ul><li>No universally agreed-upon set of roles</li><li>Items with the same role may not behave quite the same</li></ul><p>Problem avoid:</p><ul><li>Specific to individual verbs only (PropBank)</li><li>Specific to small groups of verbs (FrameNet)</li></ul><p>#2. Labelling</p><h2 id="2-1-PropBank"><a href="#2-1-PropBank" class="headerlink" title="2.1 PropBank"></a>2.1 PropBank</h2><p><strong>Frame Roles:</strong></p><ul><li>Arg0-PAG: loader</li><li>Arg1-GOL: beast of burden</li><li>Arg2-PPT: cargo</li><li>Arg3-MNR: instrument</li></ul><h2 id="2-2-FrameNet"><a href="#2-2-FrameNet" class="headerlink" title="2.2 FrameNet"></a>2.2 FrameNet</h2><p>==Meanings are reletavised to scenes!==</p><ul><li>Tries to capture relationships among word and phrase meanings by assigning them the same frame</li><li>$\approx 1000$ frames represent scenarios</li><li>Frames are explained with textual descriptions and linguistic examples</li></ul><p><strong>Resources:</strong></p><ul><li>FrameNets for several languages</li><li>Some data annotated with Frame elements from FrameNet</li><li>SEMAFOR: a frame-semantic parser</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Semantics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>15-Distributional Semantics</title>
    <link href="/2020/10/13/15-Distributional%20Semantics/"/>
    <url>/2020/10/13/15-Distributional%20Semantics/</url>
    
    <content type="html"><![CDATA[<p><strong>Distributional hypothesis:</strong></p><ul><li>Can infer meaning just by looking at the contexts a word occurs in</li><li>Meaning IS the contexts a word occurs in</li><li>either way , similar contexts imply similar meanings</li></ul><h1 id="1-Distributional-semantics"><a href="#1-Distributional-semantics" class="headerlink" title="1. Distributional semantics"></a>1. Distributional semantics</h1><h2 id="1-1-Basic-idea"><a href="#1-1-Basic-idea" class="headerlink" title="1.1 Basic idea"></a>1.1 Basic idea</h2><p>Represent each word $w_i$ as a vector of its contexts</p><p>Distributional semantic models also called vector-space models</p><h2 id="1-2-Context"><a href="#1-2-Context" class="headerlink" title="1.2 Context"></a>1.2 Context</h2><h3 id="1-2-1-Defining-the-context"><a href="#1-2-1-Defining-the-context" class="headerlink" title="1.2.1 Defining the context"></a>1.2.1 Defining the context</h3><ul><li>usually ignore stopwords</li><li>usually use a large window around the target word</li></ul><h3 id="1-2-2-Weights"><a href="#1-2-2-Weights" class="headerlink" title="1.2.2 Weights"></a>1.2.2 Weights</h3><p><strong>Mutual information:</strong><br>$$<br>\text{PMI}(x,y) = \log_2 \frac{P(x,y)}{P(x)P(y)}<br>$$</p><ul><li>PMI tells us how much more / less likely the cooccurrence is than if the words were independent</li><li><strong>Problem:</strong><ul><li>in practice, PMI is computed with counts, so it is over-sensitive to the chance co-occurrence of infrequent words</li></ul></li><li><strong>Improvement:</strong> use positive PMI (PPMI)<ul><li>change all negative PMI values to 0 (because for infrequent words, not enough data to accurately determine negative PMI values)</li><li>smoothing in PMI computation (see J &amp; M)</li></ul></li></ul><h2 id="1-3-Compositionality-in-vector-space"><a href="#1-3-Compositionality-in-vector-space" class="headerlink" title="1.3 Compositionality in vector space"></a>1.3 Compositionality in vector space</h2><p>Compositionality implies some operator $\oplus$ such that<br>$$<br>\text{meaning}(w_1w_2) = \text{meaning}(w_1) \oplus \text{meaning}(w_2)<br>$$</p><h1 id="2-Measure-similarity"><a href="#2-Measure-similarity" class="headerlink" title="2. Measure similarity"></a>2. Measure similarity</h1><ul><li>Euclidean distance</li><li>Dot product</li><li>Normalised dot product (the cosine of the angle between vectors)</li><li>Other<ul><li>Jaccard measure</li><li>Dice measure</li><li>Jenson-Shannon divergence</li><li>…</li></ul></li></ul><h1 id="3-Evaluation"><a href="#3-Evaluation" class="headerlink" title="3. Evaluation"></a>3. Evaluation</h1><ul><li>Extrinsic: IR, QA, automatic essay marking, …</li><li>Intrinsic<ul><li>relatedness judgement</li><li>word association</li></ul></li></ul><h2 id="3-1-Relatedness-judgement"><a href="#3-1-Relatedness-judgement" class="headerlink" title="3.1 Relatedness judgement"></a>3.1 Relatedness judgement</h2><p>E.g. on a scale of 1-10, how related are the two words lated?</p><p>lemon-truth = 1</p><p>lemon-orange = 10</p><ul><li>Answers depend a lot on how the question is asked</li></ul><h2 id="3-2-Word-association"><a href="#3-2-Word-association" class="headerlink" title="3.2 Word association"></a>3.2 Word association</h2><p>Say the first word that comes to ming</p><p>Collect the data and provide probabilities:</p><img src="https://waylonli.com/img/mdimg/image-20200310103655820.png" alt="image-20200310103655820" style="zoom:40%;" /><h2 id="3-3-Dimension-reduction-learn-a-more-compact-space"><a href="#3-3-Dimension-reduction-learn-a-more-compact-space" class="headerlink" title="3.3 Dimension reduction: learn a more compact space"></a>3.3 Dimension reduction: learn a more compact space</h2><ul><li>LSA (Latent Semantic Analysis)</li><li>Neural network methods</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Semantics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>14-Lexical Semantics</title>
    <link href="/2020/10/12/14-Lexical%20Semantics/"/>
    <url>/2020/10/12/14-Lexical%20Semantics/</url>
    
    <content type="html"><![CDATA[<p><strong>Goal:</strong> </p><ul><li>a machine which can answer questions in natural language</li><li>can access to KB</li><li>can access to vast quantities of English text</li></ul><h1 id="1-Semantics"><a href="#1-Semantics" class="headerlink" title="1. Semantics"></a>1. Semantics</h1><h2 id="1-1-Lexical-and-Sentential"><a href="#1-1-Lexical-and-Sentential" class="headerlink" title="1.1 Lexical and Sentential"></a>1.1 Lexical and Sentential</h2><p><strong>Lexical semantics:</strong> the meaning of individual words</p><p><strong>Sentential semantics:</strong> how word meanings combine (e.g. who did that to whom; when, how, why…)</p><h2 id="1-2-Problems"><a href="#1-2-Problems" class="headerlink" title="1.2 Problems"></a>1.2 Problems</h2><ul><li><strong>Senses:</strong> words may have different meanings (need to disambiguate between them)</li><li><strong>Synonyms:</strong> words may have the same meaning</li><li><strong>Ontology (hyponym &amp; hypernym):</strong> words can refer to a subset or superset of the concept referred to by another word (A is a B relationships)</li><li><strong>Similarity &amp; gradation:</strong> words may be related in other ways</li><li><strong>Inference</strong></li></ul><p>PS: some of these problems can be solved with a good ontology (e.g. WordNet)</p><h1 id="2-WordNet"><a href="#2-WordNet" class="headerlink" title="2. WordNet"></a>2. WordNet</h1><p>WordNet is a <strong>hand-built</strong> resource containing 117,000 synsets</p><p><strong>Relations:</strong> </p><ul><li>hyponym / hypernym (IS-A: chair-furniture)</li><li>meronym (PART-WHOLE: leg-chair)</li><li>antonym (OPPOSITES: good -bad)</li></ul><h1 id="3-Word-sense-ambiguity"><a href="#3-Word-sense-ambiguity" class="headerlink" title="3. Word sense ambiguity"></a>3. Word sense ambiguity</h1><p>Words can have multiple senses</p><p>A way to define senses: different sense = different translation</p><h2 id="3-1-Semantics-ambiguity-types"><a href="#3-1-Semantics-ambiguity-types" class="headerlink" title="3.1 Semantics ambiguity types"></a>3.1 Semantics ambiguity types</h2><ul><li><strong>Polysemy</strong>: a word has related meanings</li><li><strong>Homonymy:</strong> a word has unrelated meanings</li><li><strong>Synonymy:</strong> different words can mean the same thing</li></ul><h2 id="3-2-Word-sense-disambiguation-WSD"><a href="#3-2-Word-sense-disambiguation-WSD" class="headerlink" title="3.2 Word sense disambiguation (WSD)"></a>3.2 Word sense disambiguation (WSD)</h2><p><strong>Task:</strong> Given a sense ambiguous word, ﬁnd the sense in a given context</p><p><strong>Classifier:</strong></p><ul><li>Naive Bayes</li><li>Decision lists</li><li>Decision trees</li></ul><h3 id="3-1-1-Naive-Bayes-for-WSD"><a href="#3-1-1-Naive-Bayes-for-WSD" class="headerlink" title="3.1.1 Naive Bayes for WSD"></a>3.1.1 Naive Bayes for WSD</h3><img src="https://waylonli.com/img/mdimg/image-20200509183539534.png" alt="image-20200509183539534" style="zoom:42%;" /><h3 id="3-1-2-Features"><a href="#3-1-2-Features" class="headerlink" title="3.1.2 Features"></a>3.1.2 Features</h3><ul><li>Directly neighbouring words</li><li>Any content words in a 50 word window</li><li>Syntactically related words</li><li>Syntactic role in sense</li><li>Topic of the text</li><li>POS tag, surrounding POS tags</li></ul><h2 id="3-3-Evaluation"><a href="#3-3-Evaluation" class="headerlink" title="3.3 Evaluation"></a>3.3 Evaluation</h2><p><strong>Extrinsic:</strong> test as part of IR, QA, or MT system</p><p><strong>Intrinsic:</strong> evaluate classification accuracy or precision / recall against gold-standard senses</p><p><strong>Baseline:</strong> choose the most frequent sense</p><h2 id="3-4-Issues-with-WSD"><a href="#3-4-Issues-with-WSD" class="headerlink" title="3.4 Issues with WSD"></a>3.4 Issues with WSD</h2><ul><li>Not always clear how fine-grained the gold-standard should be</li><li>Difficult / expensive to annotate corpora with fine-grained senses</li><li>Classifier must be trained separately for each word<ul><li>hard to learn anything for <strong>infrequent or unseen words</strong> </li><li>requires new annotations for each new word</li><li>motivates unsupervised and semi-supervised methods</li></ul></li></ul><h1 id="4-Semantic-classes"><a href="#4-Semantic-classes" class="headerlink" title="4. Semantic classes"></a>4. Semantic classes</h1><p><strong>Methods:</strong> define coarse-grained semantic categories like <code>PERSON, LOCATION, ARTIFACT</code></p><ul><li><strong>Named entity recognition (NER):</strong> recognise and classify perper names in text is important for many applications</li><li>Supersense tagging</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Semantics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>13-Heads, Dependency parsing</title>
    <link href="/2020/10/11/13-Heads,%20Dpendency%20parsing/"/>
    <url>/2020/10/11/13-Heads,%20Dpendency%20parsing/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Parser-evaluation"><a href="#1-Parser-evaluation" class="headerlink" title="1. Parser evaluation"></a>1. Parser evaluation</h1><p><strong>Correct:</strong> if there is a gold constituent that spans the same sentence positions</p><p><strong>Hasher measure:</strong> also require the constituent labels to match</p><ul><li><p>precision</p></li><li><p>recall</p></li><li><p><strong>F-score (balances precision and recall):</strong><br>$$<br>\frac{2pr}{p+r}<br>$$</p></li></ul><h1 id="2-Lexicalisation"><a href="#2-Lexicalisation" class="headerlink" title="2. Lexicalisation"></a>2. Lexicalisation</h1><h2 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h2><p><strong>Problem:</strong> replacing one word with another with the same POS will never result in a different parsing decision, even though it should</p><p><strong>Solution:</strong> adding the lexical head of the phrase</p><h2 id="2-2-Pros-amp-Cons"><a href="#2-2-Pros-amp-Cons" class="headerlink" title="2.2 Pros &amp; Cons"></a>2.2 Pros &amp; Cons</h2><p><strong>Pros:</strong></p><ul><li>makes the grammar more specific</li></ul><p><strong>Cons:</strong></p><ul><li>leads to huge grammar blowup and very sparse data</li></ul><h2 id="2-3-Improvement"><a href="#2-3-Improvement" class="headerlink" title="2.3 Improvement"></a>2.3 Improvement</h2><ul><li>complex smoothing schemes (similar to N-gram interpolation / backoff)</li><li>increasing emphasis on automatically learned subcategories</li></ul><h2 id="2-4-Lexicalised-constituency-parse"><a href="#2-4-Lexicalised-constituency-parse" class="headerlink" title="2.4 Lexicalised constituency parse"></a>2.4 Lexicalised constituency parse</h2><p><strong>Direct dependency parsing:</strong></p><ul><li><p><strong>Transition-based (shift-reduce)</strong></p><p>Train a classiﬁer as the oracle to predict next action (Shift, LeftArc, or RightArc), and proceed left-to-right through the sentence</p><p>$O(n)$ time</p></li><li><p><strong>Graph-based</strong></p><p>From the fully connected directed graph of all possible edges, choose the best ones that form a tree</p><p>Edge-factored models: classiﬁer assigns a nonne g ative score to each possible edge ; maximum spanning tree algorithm ﬁnds the spanning tree with highest total score</p><p>$O(n^2)$ time</p></li></ul><h3 id="Head-Rules"><a href="#Head-Rules" class="headerlink" title="Head Rules"></a>Head Rules</h3><img src="https://waylonli.com/img/mdimg/image-20200303103851003.png" alt="image-20200303103851003" style="zoom:40%;" /><h1 id="3-Phrase-Structure-amp-Dependencies"><a href="#3-Phrase-Structure-amp-Dependencies" class="headerlink" title="3. Phrase Structure &amp; Dependencies"></a>3. Phrase Structure &amp; Dependencies</h1><p>In language with free word order, phrase structure grammars don’t make as much sense</p><img src="https://waylonli.com/img/mdimg/image-20200509163948725.png" alt="image-20200509163948725" style="zoom:40%;" /><h2 id="3-1-Pros-and-Cons"><a href="#3-1-Pros-and-Cons" class="headerlink" title="3.1 Pros and Cons"></a>3.1 Pros and Cons</h2><p><strong>Pros:</strong></p><ul><li>Sensible framework for free word order languages</li><li>Identifies syntactic relations directly</li><li>Dependency pairs / chains can make good features in classifiers, for information extraction</li><li>Parsers can be very fast</li></ul><p><strong>Cons:</strong></p><ul><li>The assumption of asymmetric binary relations is not always right (e.g. parse dogs and cats)</li></ul><h2 id="3-2-Edge-labels"><a href="#3-2-Edge-labels" class="headerlink" title="3.2 Edge labels"></a>3.2 Edge labels</h2><p><strong>Labels:</strong> subject, direct object, determiner, adjective modifier, adverbial modifier</p><img src="https://waylonli.com/img/mdimg/image-20200509164310427.png" alt="image-20200509164310427" style="zoom:40%;" /><h2 id="3-3-Dependency-Paths"><a href="#3-3-Dependency-Paths" class="headerlink" title="3.3 Dependency Paths"></a>3.3 Dependency Paths</h2><p>For information extraction tasks involving real-world relationships between entities, chains of dependencies can provide good features</p><img src="https://waylonli.com/img/mdimg/image-20200509164410511.png" alt="image-20200509164410511" style="zoom:46%;" /><p><strong>Projectivity:</strong></p><p>A sentence’s dependency parse is <strong>projective</strong> if every subtree occupies a contiguous span of the sentence</p><ul><li>= The dependency parse can be drawn on top of the sentence without any crossing edges</li></ul><p><strong>Nonprojectivity:</strong></p><ul><li>Nonprojectivity is rare in English, but quite common in many langauges</li></ul><img src="https://waylonli.com/img/mdimg/image-20200509164643457.png" alt="image-20200509164643457" style="zoom:40%;" /><h1 id="4-Dependency-Parsing"><a href="#4-Dependency-Parsing" class="headerlink" title="4. Dependency Parsing"></a>4. Dependency Parsing</h1><h2 id="4-1-CKY"><a href="#4-1-CKY" class="headerlink" title="4.1 CKY"></a>4.1 CKY</h2><p>CKY can be adapated, but efficiency is not good: obvious approach is $O(Gn^5)$; Eisner algorithm can bring it down to $O(Gn^3)$ </p><h2 id="4-2-Transition-based-Shift-reduce"><a href="#4-2-Transition-based-Shift-reduce" class="headerlink" title="4.2 Transition-based: Shift reduce"></a>4.2 Transition-based: Shift reduce</h2><p><strong>3 possible actions:</strong></p><ul><li><strong>LeftArc:</strong> assign head-dependent relation between $s_1$ and $s_2$; pop $s_2$</li><li><strong>RightArc:</strong> assign head-dependent relation between $s_2$ and $s_1$; pop $s_1$</li><li><strong>Shift:</strong> put $w_1$ on top of the stack</li></ul><h2 id="4-3-Graph-based"><a href="#4-3-Graph-based" class="headerlink" title="4.3 Graph-based"></a>4.3 Graph-based</h2><img src="https://waylonli.com/img/mdimg/image-20200509165131026.png" alt="image-20200509165131026" style="zoom:50%;" /><h2 id="4-4-Comparison"><a href="#4-4-Comparison" class="headerlink" title="4.4 Comparison"></a>4.4 Comparison</h2><img src="https://waylonli.com/img/mdimg/image-20200304233448525.png" alt="image-20200304233448525" style="zoom:46%;" /><h1 id="5-Choosing-a-Parser"><a href="#5-Choosing-a-Parser" class="headerlink" title="5. Choosing a Parser"></a>5. Choosing a Parser</h1><ul><li>Target representation: constituency or dependency?</li><li>Efficiency</li><li>Incrementality: parse the whole sentence at once, or obtain partial left-to-right analyses / expectations</li><li>Retrainable system or not</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Parsing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>11,12-Syntax and Parsing</title>
    <link href="/2020/05/11/11,12-Syntax%20and%20Parsing/"/>
    <url>/2020/05/11/11,12-Syntax%20and%20Parsing/</url>
    
    <content type="html"><![CDATA[<p>Two theories of syntax:</p><ul><li>Context-free grammar</li><li>Dependency grammar</li></ul><h1 id="1-Context-free-grammer"><a href="#1-Context-free-grammer" class="headerlink" title="1. Context-free grammer"></a>1. Context-free grammer</h1><p><strong>Symbols:</strong></p><ul><li>Terminals (t): words</li><li>Non-terminals (NT): phrasal categories (e.g. S, NP, VP, PP) with S being the start symbol</li></ul><p><strong>Chomsky Normal Form:</strong> only has rules of the form<br>$$<br>NT \rightarrow NT \ \  NT \\ \ \\<br>NT\rightarrow t<br>$$</p><h1 id="2-Syntactic-analysis"><a href="#2-Syntactic-analysis" class="headerlink" title="2. Syntactic analysis"></a>2. Syntactic analysis</h1><h2 id="2-1-Global-Ambibuity"><a href="#2-1-Global-Ambibuity" class="headerlink" title="2.1 Global Ambibuity"></a>2.1 Global Ambibuity</h2><h3 id="Structural-ambiguity"><a href="#Structural-ambiguity" class="headerlink" title="Structural ambiguity"></a>Structural ambiguity</h3><p><strong>POS ambiguity:</strong></p><img src="https://waylonli.com/img/mdimg/image-20200225102130958.png" alt="image-20200225102130958" style="zoom:50%;" /><p><strong>Attachment ambiguity:</strong> Some sentences have structural ambiguity even without part-of-speech ambiguity</p><ul><li><p>Depends on where different phrases attach in the tree</p></li><li><p>Different attachments have different meanings</p><p>I saw the man with the telescope</p><p>She ate the pizza on the floor</p><p>Good boys and girls get presents from Santa</p></li></ul><img src="https://waylonli.com/img/mdimg/image-20200225102443837.png" alt="image-20200225102443837" style="zoom:45%;" /><img src="https://waylonli.com/img/mdimg/image-20200225102610299.png" alt="image-20200225102610299" style="zoom:45%;" /><p><strong>Why we want parse trees:</strong></p><ul><li>compositional semantics: the meaning of a constituent is a function of the meaning of its children</li><li>rule-to-rule semantics: that function is determined by the rule which licenses the constituent</li></ul><p><strong>Huge problem:</strong></p><ul><li>correctness problem: need to ﬁnd the right structure to get the right meaning</li><li>efficiency problem: searching all possible structures can be very slow; want to use parsing for large-scale language tasks (for example, as used to create Google’s “infoboxes”)</li></ul><h2 id="2-2-Local-ambiguity"><a href="#2-2-Local-ambiguity" class="headerlink" title="2.2 Local ambiguity"></a>2.2 Local ambiguity</h2><p><strong>Local ambiguity is also a big problem: multiple analyses for parts of sentence.</strong> </p><ul><li>“The dog bit the child”: first three words could be NP (but aren’t)</li><li>Building useless partial structures wastes time</li><li>Avoiding useless computation is a major issue in parsing</li></ul><h1 id="3-Parsing"><a href="#3-Parsing" class="headerlink" title="3. Parsing"></a>3. Parsing</h1><h2 id="3-1-Normal-Parsing"><a href="#3-1-Normal-Parsing" class="headerlink" title="3.1 Normal Parsing"></a>3.1 Normal Parsing</h2><ul><li>Top-down: depth-first, breadth-firsts, best-first</li><li>Bottom-up</li><li>Mixed strategy (e.g. Earley algorithm)</li></ul><h3 id="3-1-1-Recursive-Descent-Parsing"><a href="#3-1-1-Recursive-Descent-Parsing" class="headerlink" title="3.1.1 Recursive Descent Parsing"></a>3.1.1 Recursive Descent Parsing</h3><p><strong>Idea:</strong> top-down, depth-first parsing</p><p>e.g. Slide 11 pp. 23-24</p><h3 id="3-1-2-Shift-reduce-Parsing"><a href="#3-1-2-Shift-reduce-Parsing" class="headerlink" title="3.1.2 Shift-reduce Parsing"></a>3.1.2 Shift-reduce Parsing</h3><p><strong>Idea:</strong> bottom-up, depth-first parsing</p><p>e.g. Slide 11 pp. 27</p><h3 id="3-1-3-CKY-algorithm-Cocke-Kasami-Younger"><a href="#3-1-3-CKY-algorithm-Cocke-Kasami-Younger" class="headerlink" title="3.1.3 CKY algorithm (Cocke, Kasami, Younger)"></a>3.1.3 CKY algorithm (Cocke, Kasami, Younger)</h3><p><strong>Idea:</strong> bottom-up, breadth-first parsing</p><p><strong>Assumption:</strong> grammar in Chomsky Normal Form</p><p><strong>Pseudocode:</strong></p><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">len</span> = <span class="hljs-number">1</span> to n:<span class="hljs-comment"># number of words in constituent</span><span class="hljs-keyword">for</span> i = <span class="hljs-number">0</span> to (n-<span class="hljs-built_in">len</span>):<span class="hljs-comment"># start position</span>    j = i + <span class="hljs-built_in">len</span><span class="hljs-comment"># end position</span>    for each A-&gt;B where c[i,j] has B:      <span class="hljs-comment"># process unary rules</span>      add A to c[i,j] <span class="hljs-keyword">with</span> a pointer to B    <span class="hljs-keyword">for</span> k = (i+<span class="hljs-number">1</span>) to (j<span class="hljs-number">-1</span>):      <span class="hljs-comment"># process binary rules</span>      for each (A-&gt;B C) where c[i,k] has B and c[k,j] has C:        add A to c[i,j] <span class="hljs-keyword">with</span> pointers to B <span class="hljs-keyword">and</span> C</code></pre><p><strong>Time complexity:</strong> $O(Gn^3)$ </p><p>e.g. Slide 11 pp.5-11</p><img src="https://waylonli.com/img/mdimg/image-20200228193756640.png" alt="image-20200228193756640" style="zoom:50%;" /><p><strong>Problem:</strong></p><ul><li>Ambiguity is only clear if we go on to reconstruct the parses using our backpointers</li></ul><p><strong>Pros &amp; Cons:</strong></p><p>Pros:</p><ul><li>much more efficient than depth-first</li></ul><p>Cons:</p><ul><li>still may compute a lot of unnecessary partial parses</li><li>requires converting the grammar to CNF</li></ul><p><strong>Can be implemented to probabilistic CKY</strong></p><h3 id="3-1-4-Choose-parsing-algorithm"><a href="#3-1-4-Choose-parsing-algorithm" class="headerlink" title="3.1.4 Choose parsing algorithm"></a>3.1.4 Choose parsing algorithm</h3><p>==Probabilities ???==</p><h2 id="3-2-Statistical-Parsing"><a href="#3-2-Statistical-Parsing" class="headerlink" title="3.2 Statistical Parsing"></a>3.2 Statistical Parsing</h2><h3 id="3-2-1-Treebank-grammars"><a href="#3-2-1-Treebank-grammars" class="headerlink" title="3.2.1 Treebank grammars"></a>3.2.1 Treebank grammars</h3><p><strong>Idea:</strong> annotate real sentences with parse trees</p><h4 id="3-2-1-1-The-Penn-Treebank-Treebank-PCFG"><a href="#3-2-1-1-The-Penn-Treebank-Treebank-PCFG" class="headerlink" title="3.2.1.1 The Penn Treebank (Treebank PCFG)"></a>3.2.1.1 The Penn Treebank (Treebank PCFG)</h4><p><strong>PCFG (probabilistic context-free grammar)</strong><br>$$<br>NT \rightarrow \beta \  \text{ with probability } P(\beta \mid NT)<br>$$</p><h4 id="3-2-1-2-Create-PCFG"><a href="#3-2-1-2-Create-PCFG" class="headerlink" title="3.2.1.2 Create PCFG"></a>3.2.1.2 Create PCFG</h4><p><strong>Easiest way: MLE</strong></p><ul><li>Count all occurrences of $NT \rightarrow \beta$ in treebank</li><li>Divide by the count of all rules whose left-hand-side is $NT$ to get $P(\beta \mid NT)$</li></ul><p>$$<br>P(NT \rightarrow C_1, C_2 \dots C_n \mid NT) = \frac{count(NT \rightarrow C_1,C_2 \dots C_n)}{count(NT)}<br>$$</p><p>e.g. Slide 12 pp. 21-24</p><h3 id="3-2-2-Probability-of-a-parse"><a href="#3-2-2-Probability-of-a-parse" class="headerlink" title="3.2.2 Probability of a parse"></a>3.2.2 Probability of a parse</h3><p>$$<br>P(t) = \prod_{NT \rightarrow \beta \in t} NT \rightarrow \beta<br>$$</p><img src="https://waylonli.com/img/mdimg/image-20200504145544413.png" alt="image-20200504145544413" style="zoom:40%;" /><h3 id="3-2-3-Probabilistic-CKY"><a href="#3-2-3-Probabilistic-CKY" class="headerlink" title="3.2.3 Probabilistic CKY"></a>3.2.3 Probabilistic CKY</h3><ul><li>When we find an $NT$ spanning $(i,j)$, store its probability along with its label in cell $(i,j)$ </li><li>If we later find an $NT$ with the same span but higher probability, replace the probability and the backpointers for $NT$ in cell $(i,j)$ </li></ul><h3 id="3-2-4-Other-algorithms"><a href="#3-2-4-Other-algorithms" class="headerlink" title="3.2.4 Other algorithms"></a>3.2.4 Other algorithms</h3><ul><li>Inside algorithm</li><li>Inside-outside algorithm</li></ul><h3 id="3-2-5-Best-first-probabilistic-parsing"><a href="#3-2-5-Best-first-probabilistic-parsing" class="headerlink" title="3.2.5 Best-first probabilistic parsing"></a>3.2.5 Best-first probabilistic parsing</h3><p>Basic idea: use probabilities of subtrees to decide which ones to build up further</p><img src="https://waylonli.com/img/mdimg/image-20200504164535795.png" alt="image-20200504164535795" style="zoom:50%;" /><p><strong>How to score constituents:</strong></p><ul><li><p>Why not probability: smaller constituents will almost always have higher scores</p></li><li><p>Possible solution:</p><ul><li>divide by the number of words (but not guaranteed to find the best parse first)</li><li>A*</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Parsing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10-Annotation &amp; Evaluation</title>
    <link href="/2020/03/26/10-Annotation-and-Evaluation/"/>
    <url>/2020/03/26/10-Annotation-and-Evaluation/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Annotation"><a href="#1-Annotation" class="headerlink" title="1. Annotation"></a>1. Annotation</h1><p>Penn Treebank: 36 POS tags (excluding punctuation)</p><p><strong>Factors in Annotation:</strong> </p><ul><li>source data (genre? size? licensing?)</li><li>annotation scheme (complexity? guidelines?)</li><li>annotators (expertise? training?)</li><li>annotation software (graphical interface?)</li><li>quality control procedures (multiple annotation, adjudication?)</li></ul><!-- more  --><h1 id="2-Evaluation"><a href="#2-Evaluation" class="headerlink" title="2. Evaluation"></a>2. Evaluation</h1><h2 id="2-1-Hypotheses"><a href="#2-1-Hypotheses" class="headerlink" title="2.1 Hypotheses"></a>2.1 Hypotheses</h2><p>About existing linguistic objects:</p><ul><li>Is this text by Shakespeare or Marlowe?</li></ul><p>About output of a language system:</p><ul><li>How well does this language model predict the data?</li><li>How accurate is this segmenter / tagger / parser?<ul><li>Is this segmenter / tagger / parser better than that one?</li></ul></li></ul><p>About human beings:</p><ul><li>How reliable is this person’s annotation?</li><li>To what extent do these two annotators agree? ( IAA )</li></ul><h2 id="2-2-Gold-Standard-Evaluation"><a href="#2-2-Gold-Standard-Evaluation" class="headerlink" title="2.2 Gold Standard Evaluation"></a>2.2 Gold Standard Evaluation</h2><ul><li>Gold standards used both for training and for evaluation</li><li>But testing must be done on unseen data</li></ul><h2 id="2-3-Tuning"><a href="#2-3-Tuning" class="headerlink" title="2.3 Tuning"></a>2.3 Tuning</h2><ul><li>Lidstone (add-$\lambda$) smoothing</li><li>Choosing features for text classification</li></ul><p><strong>Overfitting:</strong> the test set is no longer a reliable proxy for new data.</p><p><strong>Solution:</strong> hold out a second set for tuning, called a <strong>development</strong> set, save the test set for the very end</p><h2 id="2-4-Cross-validation"><a href="#2-4-Cross-validation" class="headerlink" title="2.4 Cross-validation"></a>2.4 Cross-validation</h2><p>Use when the dataset is too small to have a nice train / test or train / dev / test split</p><h3 id="k-fold-cross-validation"><a href="#k-fold-cross-validation" class="headerlink" title="$k$-fold cross-validation"></a>$k$-fold cross-validation</h3><p>Partition the data into $k$ pieces and treat them as mini held-out sets. Each fold is an experiment with a diﬀerent held-out set, using the rest of the data for training</p><p>PS: If tuning the system via cross-validation, still important to have a separate blind test set.</p><h2 id="2-5-Measurement"><a href="#2-5-Measurement" class="headerlink" title="2.5 Measurement"></a>2.5 Measurement</h2><p>$$<br>\text{precision} = \frac{TP}{TP+FP} \\ \ \\<br>\text{recall (true positive rate)} = \frac{TP}{TP+FN}\\ \ \\<br>F_1 = \  \frac{\text{precision} \cdot \text{recall}}{\text{precision}+\text{recall}}<br>$$</p><p><strong>Upper bound:</strong> Turing test</p><ul><li>When using a human Gold Standard, check the agreement of humans against that standard.</li></ul><p><strong>Lower bound:</strong> performance of baseline model</p><p><strong>Confusion matrix</strong> </p>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Evaluation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-Time Speech Recognition Using Python</title>
    <link href="/2020/03/05/Real-Time-Speech-Recognition-Using-Python/"/>
    <url>/2020/03/05/Real-Time-Speech-Recognition-Using-Python/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Environment"><a href="#1-Environment" class="headerlink" title="1. Environment"></a>1. Environment</h1><h2 id="1-1-Pyaudio"><a href="#1-1-Pyaudio" class="headerlink" title="1.1 Pyaudio"></a>1.1 Pyaudio</h2><a id="more"></a><ul><li>Windows: <code>pip install pyaudio</code></li><li>Linux: <code>sudo apt-get install python-pyaudio python3-pyaudio</code> </li><li>Mac OSX: <code>brew install portaudio</code> (need to <a href="https://osxdaily.com/2018/03/07/how-install-homebrew-mac-os/">install Homebrew</a> on your mac first); then <code>pip install pyaudio</code></li></ul><h2 id="1-2-SpeechRecognition-package"><a href="#1-2-SpeechRecognition-package" class="headerlink" title="1.2 SpeechRecognition package"></a>1.2 SpeechRecognition package</h2><p><code>pip install SpeechRecognition</code></p><p><a href="https://pypi.org/project/SpeechRecognition/"><strong>Package webpage</strong></a></p><h1 id="2-Example-code"><a href="#2-Example-code" class="headerlink" title="2. Example code"></a>2. Example code</h1><p>Here is an example for recognising ‘yes’ and ‘no’:</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> speech_recognition <span class="hljs-keyword">as</span> sr<span class="hljs-keyword">import</span> os<span class="hljs-comment"># obtain audio from the microphone</span>r = sr.Recognizer()t = <span class="hljs-literal">True</span><span class="hljs-keyword">while</span> t:    <span class="hljs-keyword">with</span> sr.Microphone() <span class="hljs-keyword">as</span> source:        r.adjust_for_ambient_noise(source, duration = <span class="hljs-number">0.5</span>)          print(<span class="hljs-string">&#x27;say something&#x27;</span>)        audio = r.record(source, duration = <span class="hljs-number">2</span>)    <span class="hljs-comment"># listen for 2 seconds</span>        output = r.recognize_google(audio, show_all = <span class="hljs-literal">True</span>)    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">len</span>(output) &lt; <span class="hljs-number">1</span>):        print(<span class="hljs-string">&quot;Say louder&quot;</span>)    <span class="hljs-comment"># if the recogniser did not recognise anything from the microphone, ask speaker to say louder</span>    <span class="hljs-keyword">else</span>:        possible = [word[<span class="hljs-string">&#x27;transcript&#x27;</span>] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> output[<span class="hljs-string">&#x27;alternative&#x27;</span>]]    <span class="hljs-comment"># extract all the possible phrase from return dictionary</span>        <span class="hljs-keyword">if</span> (<span class="hljs-string">&quot;yes&quot;</span> <span class="hljs-keyword">in</span> possible):            print(<span class="hljs-string">&quot;yes&quot;</span>)            t = <span class="hljs-literal">False</span>        <span class="hljs-keyword">if</span> (<span class="hljs-string">&quot;no&quot;</span> <span class="hljs-keyword">in</span> possible):            print(<span class="hljs-string">&quot;no&quot;</span>)            t = <span class="hljs-literal">False</span>        <span class="hljs-keyword">else</span>:            print(<span class="hljs-string">&quot;Say it again&quot;</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Tutorials</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Speech-Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Interesting exploration of i++ and ++i in Java</title>
    <link href="/2020/02/24/Interesting-exploration-of-i-and-i-in-Java/"/>
    <url>/2020/02/24/Interesting-exploration-of-i-and-i-in-Java/</url>
    
    <content type="html"><![CDATA[<p>An interesting discover of using <code>i++</code> and <code>++i</code> in Java loop</p><a id="more"></a><p>In general cases,</p><ul><li><code>i++</code> in Java means use the value of <code>i</code> and increase it by 1 after finish.</li><li><code>++i</code> in Java means firstly increase <code>i</code> by 1 and use it afterwards</li></ul><p>E.g.</p><pre><code class="hljs java"><span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>;<span class="hljs-keyword">int</span> output1 = i++;i = <span class="hljs-number">1</span>;<span class="hljs-keyword">int</span> output2 = ++i;System.out.println(output1);System.out.println(output2);</code></pre><p>Output:</p><pre><code class="hljs java"><span class="hljs-number">1</span><span class="hljs-number">2</span></code></pre><p>However, in for loop:</p><pre><code class="hljs java"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= <span class="hljs-number">5</span>; i++) &#123;System.out.println(i);&#125;</code></pre><p>And:</p><pre><code class="hljs java"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= <span class="hljs-number">5</span>; ++i) &#123;System.out.println(i);&#125;</code></pre><p>The output are both:</p><pre><code class="hljs java"><span class="hljs-number">1</span><span class="hljs-number">2</span><span class="hljs-number">3</span><span class="hljs-number">4</span><span class="hljs-number">5</span></code></pre><p>These two methods can basically be rewritten as:</p><pre><code class="hljs java"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= <span class="hljs-number">5</span>;) &#123;System.out.println(i);i++; <span class="hljs-comment">// or ++i</span>&#125;</code></pre><p>After searching for reasons, a developer introduces in a blog that though the output for using <code>i++</code> and <code>++i</code> are the same, <code>i++</code> needs a temporary variable to store the origin value before increasing, while there is no such a process in <code>++i</code>. Therefore, the running time of <code>i++</code> is longer than <code>++i</code>, which is shown in the blog.</p><p>This shows that we should use <code>++i</code> instead of <code>i++</code>, which can somehow improve the performance.</p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>8,9-POS tagging and HMMs</title>
    <link href="/2020/02/11/8,9-POS-tagging-and-HMMs/"/>
    <url>/2020/02/11/8,9-POS-tagging-and-HMMs/</url>
    
    <content type="html"><![CDATA[<p>Use Hidden Markov Models to do POS tagging</p><!-- more  --><p><strong>Notation:</strong></p><ul><li><p>Sequence of observation overtime (sentence): $ O=o_1\dots o_T $ </p></li><li><p>Sequence of states (tags): $Q=q^1\dots q^N$</p><p>Sequence states over time: $Q = q_1 \dots q_T$ </p></li><li><p>Vocabulary size: $V$ </p></li><li><p>Time step: $t$, not a tag</p></li><li><p>Matrix of transition probabilities: $A$ </p><ul><li>$a_{ij}$: the prob of transitioning from state $q^i$ to $q^j$ </li></ul></li><li><p>Matrix of output probabilities: $B$ </p><ul><li>$b_i(o_t)$: the prob of emitting $o_t$ from state $q^i$ </li></ul></li></ul><h1 id="1-Pos-tagging"><a href="#1-Pos-tagging" class="headerlink" title="1. Pos tagging"></a>1. Pos tagging</h1><h2 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h2><p><strong>Uses:</strong></p><ul><li>First step towards syntactic analysis</li><li>Often useful for semantic analysis</li><li>Illustrate the use of HMMs</li></ul><p><strong>Depends on:</strong></p><ul><li>The word to be labeled</li><li>The labels of surrounding words</li></ul><p><strong>Tags:</strong> Penn Treebank POS tags</p><img src="https://waylonli.com/img/mdimg/image-20200205210126948.png" alt="image-20200205210126948" style="zoom:43%;" /><p><strong>Difficulties:</strong></p><ul><li>Ambiguity</li><li>Sparse data</li></ul><h2 id="1-2-Probabilistic-model-for-tagging-forward-algorithm"><a href="#1-2-Probabilistic-model-for-tagging-forward-algorithm" class="headerlink" title="1.2 Probabilistic model for tagging (forward algorithm?)"></a>1.2 Probabilistic model for tagging (forward algorithm?)</h2><p><strong>Assumption:</strong></p><ul><li>Each tag depends only on previous tag (bigram tag model)</li><li>Words are independent given tags</li></ul><p><strong>Finite-state machine:</strong> sentences are generated by walking through states in a graph, each state represents a tag</p><p><strong>Given:</strong></p><ul><li>The transition and output proabilities</li></ul><p>$$<br>P(O,Q) = \prod_{t=1}^n P(o_t \mid q_t) P(q_t \mid q_{t-1})<br>$$</p><h1 id="2-HMM"><a href="#2-HMM" class="headerlink" title="2. HMM"></a>2. HMM</h1><h2 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h2><p><strong>Purpose:</strong> find the best tag sequence for an untagged sentence</p><ul><li>Markov: Markov independence assumption (each tag / state only depends on fixed number of previous tags / states)</li><li>Hidden: at test time we only see the words / emissions, the tags / states are hidden variables</li></ul><p><strong>Elements:</strong></p><ul><li>a set of states (e.g. tags)</li><li>a set of output symbol (e.g. words)</li><li>initial state (e.g. beginning of the sentence</li><li>state transition probabilities (e.g. $P(t_i \mid t_{i-1})$)</li><li>symbol emission probabilities (e.g. $P(w_i \mid t_i)$)</li></ul><h2 id="2-2-Model"><a href="#2-2-Model" class="headerlink" title="2.2 Model"></a>2.2 Model</h2><p>$$<br>\underset{Q}{\operatorname{argmax}} P(Q \mid O) = \underset{Q}{\operatorname{argmax}} P(O \mid Q) P(Q)<br>$$</p><ul><li>Using Bayes’ rule</li><li>$P(Q) = \prod_t P(q_t \mid q_{t-1})$</li><li>$P(O \mid Q) = \prod_t P(o_t \mid q_t)$ </li></ul><p><strong>Joint probability:</strong><br>$$<br>P(O,Q\mid \lambda) = \prod_{t=1}^T P(o_t \mid q_t) P(q_{t-1}) = \prod_{t=1}^T b_{t}(o_t)a_{t-1,t}<br>$$</p><ul><li>$\lambda = (A,B)$ </li></ul><h2 id="2-3-Transition-and-Output-Probabilities-Matrix-A-B"><a href="#2-3-Transition-and-Output-Probabilities-Matrix-A-B" class="headerlink" title="2.3 Transition and Output Probabilities (Matrix $A,B$)"></a>2.3 Transition and Output Probabilities (Matrix $A,B$)</h2><ul><li>Transition matrix $A$: $a_{ij} = P(q_j \mid q_i)$ </li><li>Output matrix $B$: $b_i(o)=P(o \mid q^i)$ for output $o$ </li></ul><h2 id="2-4-Searching-Viterbi-algorithm"><a href="#2-4-Searching-Viterbi-algorithm" class="headerlink" title="2.4 Searching: Viterbi algorithm"></a>2.4 Searching: Viterbi algorithm</h2><p><strong>Decoding:</strong> finding the best tag sequence for a sentence is called decoding</p><p><strong>Intuition:</strong> the best path of length $t$ ending in state $Q$ must include the best path of length $t-1$ to the previous state</p><p><strong>Find $\underset{Q}{\operatorname{argmax}} P(Q \mid O) $:</strong><br>$$<br>v(j,t) = \max_{i=1}^N v(i,t-1) \cdot a_{ij} \cdot b_j<br>$$</p><ul><li><strong>Can also use negative log probabilities and take minimum over sum of costs</strong> </li></ul><p><strong>Chart:</strong> $N \times T$ </p><img src="https://waylonli.com/img/mdimg/image-20200210220921409.png" alt="image-20200210220921409" style="zoom:35%;" /><h2 id="2-5-Training-Baum-Welch-Forward-Backward-algorithm"><a href="#2-5-Training-Baum-Welch-Forward-Backward-algorithm" class="headerlink" title="2.5 Training: Baum-Welch (Forward-Backward) algorithm"></a>2.5 Training: Baum-Welch (Forward-Backward) algorithm</h2><p><strong>Compute the best parameters from unannotated corpus</strong></p><h3 id="2-5-1-Expectation-maximisation"><a href="#2-5-1-Expectation-maximisation" class="headerlink" title="2.5.1 Expectation-maximisation"></a>2.5.1 Expectation-maximisation</h3><ul><li>Initialise parameters $\lambda^{(0)}$ </li><li>At each iteration $k$,<ul><li>E-step: compute expected counts using $\lambda^{(k-1)}$ </li><li>M-step: set $\lambda^{(k)}$ using MLE on the expected counts</li></ul></li><li>Repeat until $\lambda$ does not change</li></ul><p>PS:</p><ul><li>Can find a local maximum, not guarantee to find the global maximum</li></ul><h3 id="2-5-2-Baum-Welch"><a href="#2-5-2-Baum-Welch" class="headerlink" title="2.5.2 Baum-Welch"></a>2.5.2 Baum-Welch</h3><p><strong>Idea:</strong> computes expected counts using forward probabilities and backward probabilities<br>$$<br>\beta(j,t) = P(q^t = j, o_{t+1}, o_{t+2}, \dots,o_T \mid \lambda)<br>$$<br>PS: EM is much more general, can use for many latent variable models</p><h2 id="2-6-Evaluation-Forward-algorithm-sum-instead-of-max-compute-the-likelihood"><a href="#2-6-Evaluation-Forward-algorithm-sum-instead-of-max-compute-the-likelihood" class="headerlink" title="2.6 Evaluation: Forward algorithm (sum instead of max): compute the likelihood"></a>2.6 Evaluation: Forward algorithm (sum instead of max): compute the likelihood</h2><p>$$<br>a(j,t) = \sum_{i=1}^N a(i,t-1)\cdot a_{ij}\cdot b_j(o_t)<br>$$</p><ul><li>Actually a language model, not just count for the one previous step, but also all the earlier steps</li></ul><h2 id="2-7-Three-main-problems"><a href="#2-7-Three-main-problems" class="headerlink" title="2.7 Three main problems"></a>2.7 Three main problems</h2><ol><li><strong>Evaluation problem</strong><ul><li>Evaluation problem answers the question: what is the probability that a particular sequence of symbols is produced by a particular model?</li><li>For evaluation we use two algorithms: the <em>forward algorithm</em> or the <em>backwards algorithm</em> (DO NOT confuse them with the forward-backward algorithm).</li></ul></li><li><strong>Decoding problem</strong><ul><li>Decoding problem answers the question: Given a sequence of symbols (your observations) and a model, what is the most likely sequence of states that produced the sequence.</li><li>For decoding we use the <em>Viterbi algorithm</em>.</li></ul></li><li><strong>Training problem</strong><ul><li>Training problem answers the question: Given a model structure and a set of sequences, find the model that best fits the data.</li><li>For this problem we can use the following 3 algorithms:<ol><li>MLE (maximum likelihood estimation)</li><li>Viterbi training(DO NOT confuse with Viterbi decoding)</li><li>Baum Welch = forward-backward algorithm</li></ol></li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>POS-Tagging</tag>
      
      <tag>HMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Structural Testing</title>
    <link href="/2020/02/05/Structural-Testing/"/>
    <url>/2020/02/05/Structural-Testing/</url>
    
    <content type="html"><![CDATA[<p><strong>Def:</strong> judging test suite thoroughness based on the structure of the program itself</p><ul><li>Also known as <strong>white-box testing</strong></li><li>Distinguish from functional testing (black-box testing)<ul><li>structural testing is still testing product functionality against its specification</li><li>only the measure of thoroughness has changed</li></ul></li></ul><p><strong>Why need structural testing:</strong></p><ul><li>Executing all control flow elements does not guarantee finding all faults</li><li>Increase confidence in thoroughness of testing</li></ul><p><strong>Practical use:</strong></p><ul><li>Firstly create functional test suite</li><li>Then measure structural coverage to identify see what is missing</li></ul><h2 id="Statement-testing-amp-Branch-testing"><a href="#Statement-testing-amp-Branch-testing" class="headerlink" title="Statement testing &amp; Branch testing"></a>Statement testing &amp; Branch testing</h2><p><strong>Statement testing:</strong></p><ul><li>Adequacy criterion: each statement must be executed at least once</li><li>Rationale: a fault in a statement can only be revealed by executing the faulty statement</li></ul><p><strong>Branch testing:</strong></p><ul><li>Adequacy criterion: each branch (edge in the CFG) must be executed at least once</li></ul><p><strong>Comparison:</strong></p><p>Traversing all edges of a graph causes all nodes to be visited, so we have:</p><ul><li>Test suites that satisfy the branch adequacy criterion for a program P also satisfy the statement adequacy criterion for the same program</li><li>A statement-adequate (or node-adequate) test suite may not be branch-adequate (edge-adequate)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Software-Testing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Software-Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>7-Text Classification</title>
    <link href="/2020/02/04/7-Text-classification/"/>
    <url>/2020/02/04/7-Text-classification/</url>
    
    <content type="html"><![CDATA[<p>Introduce two different methods to do text classification: Naive Bayes and Maximum Entropy</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><strong>Application:</strong></p><ul><li>spam detection (binary: spam / not spam)</li><li>sentiment analysis (binary or multiway)<ul><li>review (pos / neg or 1-5 stars)</li><li>political argument (pro / con, or pro / con / neutral)</li></ul></li><li>topic classification (multiway: sport / finance / travel, etc.)</li><li>categorise the author<ul><li>native language identification</li><li>diagnosis of disease</li><li>gender, dialect, educational background, political, orientation</li></ul></li></ul><p><strong>Why not N-gram:</strong></p><ul><li>sequential relationships are largely irrelevant for many tasks (bag of words assumption)</li><li>want to include other features (e.g. POS tags) that N-gram models do not include</li></ul><p><strong>Two methods:</strong></p><ul><li>Naive Bayes</li><li>Maximum Entropy (multinomial logistic regression)</li></ul><h1 id="2-Naive-Bayes"><a href="#2-Naive-Bayes" class="headerlink" title="2. Naive Bayes"></a>2. Naive Bayes</h1><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p>$$<br>\hat{c} = \underset{c\in C}{\operatorname{argmax}} P(c\mid d) = \underset{c\in C}{\operatorname{argmax}} \frac{P(d \mid c)P(c)}{P(d)} = \underset{c\in C}{\operatorname{argmax}}P(d \mid c)P(c)<br>$$</p><ul><li><p>Naive Bayes assumption: $P(d \mid c) = P(f_1, f_2,\dots ,f_n \mid c) \approx P(f_1 \mid c)P(f_2\mid c)\dots P(f_n \mid c)$<br>$$<br>\ \\ \hat{P}(f_i \mid c) = \frac{\text{count}(f_i,c)+\alpha}{\sum_{f \in F}(\text{count}(f,c)+\alpha)}<br>$$<br>PS: $\alpha$ needs to be added in calculations for every class, so the total count should add $\left |F\right |*\alpha$ </p><ul><li>$F$: the set of possible features</li><li>$\alpha$: smoothing parameter</li><li>all features not shown earlier before have $\hat{P}(f_i \mid +) = \frac{\alpha}{68+\alpha \left |F\right |}$ </li></ul></li><li><p>$P(c)$: estimated with MLE<br>$$<br>\hat{P}(c) = \frac{N_c}{N}<br>$$</p></li></ul><h2 id="2-2-Step"><a href="#2-2-Step" class="headerlink" title="2.2 Step"></a>2.2 Step</h2><ul><li><p>Test document $d$ </p></li><li><p>Calculate<br>$$<br>P(+ \mid d) \propto P(+) \prod_{i=1}^n P(f_i \mid +) \\ \  \\ P(-\mid d) \propto P(-) \prod_{i=1}^n P(f_i \mid -)<br>$$</p></li><li><p>Choose the one with larger value</p></li></ul><h2 id="2-3-Choose-features"><a href="#2-3-Choose-features" class="headerlink" title="2.3 Choose features"></a>2.3 Choose features</h2><ul><li>Binary value for $f_i$</li><li>Use subset of $\left | F \right |$ (e.g. ignore stop words)</li><li>Use more complex features (e.g. bigrams, etc.)</li></ul><h2 id="2-4-Deal-with-very-small-numbers-underflow-use-costs"><a href="#2-4-Deal-with-very-small-numbers-underflow-use-costs" class="headerlink" title="2.4 Deal with very small numbers (underflow): use costs"></a>2.4 Deal with very small numbers (underflow): use costs</h2><p>Find the lowest cost classification<br>$$<br>\hat{c} = \underset{c\in C}{\operatorname{argmin}}  (-\log P(c) + \sum_{i=1}^n - \log P(f_i \mid c))<br>$$</p><ul><li>Costs are negative log probabilities</li><li>Can avoid underflow by turning multiplication to sum</li><li>Look for the lowest cost overall</li></ul><h2 id="2-5-Implementation"><a href="#2-5-Implementation" class="headerlink" title="2.5 Implementation"></a>2.5 Implementation</h2><ul><li>Naive Bayes is a <strong>linear classifier</strong> since it uses a  linear function of the input feature</li><li>As is logistic regression</li></ul><h2 id="2-6-Problems-Pros-amp-Cons"><a href="#2-6-Problems-Pros-amp-Cons" class="headerlink" title="2.6 Problems, Pros &amp; Cons"></a>2.6 Problems, Pros &amp; Cons</h2><p><strong>Problem:</strong></p><ul><li><p>might need domain-specific non-sentiment words</p><p>e.g. memory, CPU for computer product reviews</p></li><li><p>stopwords might be very useful features for some tasks</p></li><li><p>probably better to use too many irrelevant features than not enough relevant features</p></li></ul><p><strong>Pros:</strong></p><ul><li>Easy to implement</li><li>Fast to train</li><li>Do not need much training data compared with other methods (good for small datasets)</li><li>Usually works reasonably well</li><li>Can be a baseline method for any classification task</li></ul><p><strong>Cons:</strong></p><ul><li>Assumption is naive!<ul><li>features are not usually independent given the class</li><li>adding feature types often leads to even stronger correlations between features</li><li>accuracy can somtimes be OK, but will be highly <strong>overconfident</strong> in its decisions</li></ul></li></ul><h1 id="3-Maximum-Entropy-Model"><a href="#3-Maximum-Entropy-Model" class="headerlink" title="3. Maximum Entropy Model"></a>3. Maximum Entropy Model</h1><h2 id="3-1-Model"><a href="#3-1-Model" class="headerlink" title="3.1 Model"></a>3.1 Model</h2><p>$$<br>P(c \mid \vec{x}) = \frac{1}{Z}\exp(\sum_i w_i f_i(\vec{x},c)) \\ \ \\ \text{where } Z = \sum_{c’} \exp(\sum_iw_i f_i(\vec{x},c))<br>$$</p><ul><li><strong>Multinomial logistic regression</strong>, model $P(c \mid d)$ directly instead of using Bayes’ rule</li><li>Use vector to represent the observed data</li><li>End up choosing the class for whose $\vec{w} \cdot \vec{f}$ is the highest</li></ul><p>PS:</p><ul><li>For each class, $\vec{w}$ varies since each word cause different influence on the class</li></ul><h2 id="3-2-Training"><a href="#3-2-Training" class="headerlink" title="3.2 Training"></a>3.2 Training</h2><p><strong>Conditional maximum likelihood estimation (CMLE):</strong></p><p>Given instances $x^{(1)} \dots x^{(N)}$ with labels $c^{(1)}\dots c^{(N)}$<br>$$<br>\vec{w} = \underset{\vec{w}}{\operatorname{argmax}} \sum_j \log P(c^{(j)}\mid x^{(j)})<br>$$<br><strong>Avoid overfitting:</strong> regularisation</p><h2 id="3-3-Pros-amp-Cons"><a href="#3-3-Pros-amp-Cons" class="headerlink" title="3.3 Pros &amp; Cons"></a>3.3 Pros &amp; Cons</h2><p><strong>Pros:</strong></p><ul><li>Better performance</li><li>Do not need conditionally independent assumption</li></ul><p><strong>Cons:</strong></p><ul><li><strong>Time-consuming</strong> if there are a large number of classes and / or thousands of features to extract from each training example</li></ul><h2 id="3-4-Why-is-called-MaxEnt"><a href="#3-4-Why-is-called-MaxEnt" class="headerlink" title="3.4 Why is called MaxEnt?"></a>3.4 Why is called MaxEnt?</h2><ul><li>The probabilistic model we are building should follow whatever constraints we impose on it</li><li>But beyond these constraints we should follow Occam’s Razor, make the fewest possible assumptions</li></ul><h1 id="4-Summary"><a href="#4-Summary" class="headerlink" title="4. Summary"></a>4. Summary</h1><h2 id="4-1-Differences"><a href="#4-1-Differences" class="headerlink" title="4.1 Differences"></a>4.1 Differences</h2><table><thead><tr><th align="center"></th><th align="center">Naive Bayes</th><th align="center">MaxEnt</th></tr></thead><tbody><tr><td align="center">Model</td><td align="center">MLE (maximum likelihood estimation)<br />Bayes’ rule</td><td align="center">CMLE (conditional maximum likelihood estimation)<br />Multinomial logistic regression</td></tr><tr><td align="center">Features</td><td align="center">Directly observed</td><td align="center">Use vector representation<br />Features are functions that depend on both observation $\vec{x}$ and class $c$</td></tr><tr><td align="center">Assumption</td><td align="center">Conditionally independent assumption</td><td align="center">None</td></tr></tbody></table><h2 id="4-2-Similarity"><a href="#4-2-Similarity" class="headerlink" title="4.2 Similarity"></a>4.2 Similarity</h2><ul><li>Both are easily available in standard ML toolkits</li><li>Both need to choose what features are good to use </li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Naive-Bayes</tag>
      
      <tag>Maximum-Entropy-Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>6-Spelling correction, edit distance and EM</title>
    <link href="/2020/01/30/6-Spelling-correction-edit-distance-and-EM/"/>
    <url>/2020/01/30/6-Spelling-correction-edit-distance-and-EM/</url>
    
    <content type="html"><![CDATA[<p>Introduction to how to do spelling correction</p><a id="more"></a><h1 id="1-Spelling-correction"><a href="#1-Spelling-correction" class="headerlink" title="1 Spelling correction"></a>1 Spelling correction</h1><p>##1.1 Simple version</p><p><strong>Assumption:</strong></p><ul><li>have a large dictionary of real word</li><li>only correct non-word</li><li>only consider corrections differ by a <strong>single</strong> character from the non-word</li></ul><p><strong>Idea:</strong></p><ul><li>generate a list of all words $y$ differ by 1 character from $x$</li><li>compute the probability of each $y$ and choose the highest one</li></ul><h2 id="1-2-Alignments-and-edit-distance"><a href="#1-2-Alignments-and-edit-distance" class="headerlink" title="1.2 Alignments and edit distance"></a>1.2 Alignments and edit distance</h2><p><strong>Edit distance:</strong></p><ul><li>substitution</li><li>deletion</li><li>insertion</li></ul><p><strong>Need to:</strong></p><ul><li><p>find the optimal character alignment between two words</p><p>the optimal alignment is the one with <strong>minimum edit distance (MED)</strong></p></li></ul><p><strong>How:</strong></p><ul><li>Dynamic programming: Viterbi, CKY</li></ul><p><strong>Uses:</strong></p><ul><li>spelling correction</li><li>morphological analysis</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Edit-distance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5-Distributed representations</title>
    <link href="/2020/01/28/5-Distributed-representations/"/>
    <url>/2020/01/28/5-Distributed-representations/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to word embedding</p><a id="more"></a><h1 id="1-Distributed-word-representation-Word2Vec"><a href="#1-Distributed-word-representation-Word2Vec" class="headerlink" title="1 Distributed word representation (Word2Vec)"></a>1 Distributed word representation (Word2Vec)</h1><p>Can represent the similarity of two words</p><p><strong>Early:</strong> </p><ul><li>define classes $c$ of words, by hand or automatically</li><li>$P_{CL}(w_i \mid w_{i-1})=P(c_i \mid c_{i-1})P(w_i \mid c_i)$</li></ul><p><strong>Recent idea:</strong> use neural networks to project words into a continuous space, so words that appear in similar contexts have similar representation (usually 50 or 100 dimensions)</p><p><strong>Pros:</strong></p><ul><li>can represent the similarity (distance) between different words</li></ul><h2 id="1-1-How-to-train"><a href="#1-1-How-to-train" class="headerlink" title="1.1 How to train?"></a>1.1 How to train?</h2><p>No direct way to train word vectors, but we can get word vectors while training our language model</p><p><strong>Models for one-hot representation:</strong> neural network language model (NNLM), recurrent neural network language model (RNNLM)</p><ul><li>calculations are too complicated</li><li>too many parameters</li></ul><p><strong>Models for Word2Vec:</strong> </p><ul><li>Continuous Bag of Words (CBOW): predict central word based on the context</li><li>Skip-gram: predict context based on the central word</li></ul><h3 id="1-1-1-CBOW"><a href="#1-1-1-CBOW" class="headerlink" title="1.1.1 CBOW"></a>1.1.1 CBOW</h3><img src = "https://waylonli.com/img/mdimg/image-20200128213558577.png" style="zoom:50%;"/><p><strong>Loss function:</strong><br>$$<br>L=\sum_{w \in C} \log P(w \mid \text{Context}(w))<br>$$</p><ul><li>no hidden layer (just one projection layer)</li><li>two directions window</li><li>input: one-hot representations</li><li>simplify projection layer as a sum and a mean operation</li></ul><p>After getting the weights matrix $w$, multiply one-hot representation and $w$, we can get the distributed representation of that word</p><ul><li>accelerating method: Hierarchical softmax, Negative sampling</li></ul><h3 id="1-1-2-Skip-gram"><a href="#1-1-2-Skip-gram" class="headerlink" title="1.1.2 Skip-gram"></a>1.1.2 Skip-gram</h3><img src = "https://waylonli.com/img/mdimg/image-20200128214415619.png" style="zoom:50%;"/><p><strong>Loss function:</strong><br>$$<br>L=\sum_{w \in C} \log P(\text{context}(w) \mid w)<br>$$</p><ul><li><p>two parameters</p><ul><li><p>skip_window: window size = 2*skip_window + 1</p></li><li><p>num_skips: pick num_skips pairs of words</p><p>e.g. I am a boy who loves football.</p><p>for num_skips = 2, skip_window = 2, word = a</p><p>=&gt; (a, I), (a, am), (a,boy), (a,who)</p></li></ul></li><li><p>input: one-hot representation of $w(t)$ </p></li></ul><p>##1.2 Problem</p><ul><li>the use of window size ignore the information contained in the whole text</li><li>Word vector cannot represent different meanings of a single word</li></ul><p>Solution: Glove</p><h1 id="2-Noisy-channel-model"><a href="#2-Noisy-channel-model" class="headerlink" title="2 Noisy channel model"></a>2 Noisy channel model</h1><img src = "https://waylonli.com/img/mdimg/image-20200128104730602.png" style="zoom:50%;"/>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Word2Vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4-Evaluation and smoothing</title>
    <link href="/2020/01/23/4-Evaluation-and-smoothing/"/>
    <url>/2020/01/23/4-Evaluation-and-smoothing/</url>
    
    <content type="html"><![CDATA[<p>Introduction to smoothing word counts to get the probabilities</p><a id="more"></a><h1 id="1-Evaluation"><a href="#1-Evaluation" class="headerlink" title="1. Evaluation"></a>1. Evaluation</h1><ul><li>Extrinstic: measure performance on a downstream application<ul><li>plug different models into the system, test on the same dataset and compare the accuracy</li></ul></li><li>Intrinstic: design a measure that is inherent to the current task</li></ul><p><strong>Entropy:</strong> human need $H(X)$ of Y/N questions to determine the next word<br>$$<br>H(X)=\sum_x -P(x) \log_2P(x)<br>$$<br><strong>Cross-entropy:</strong><br>$$<br>H(P,\hat{P}) = \sum_x -P(x) \log_2 \hat{P}(x)<br>$$</p><ul><li>cross-entropy $\ge$ entropy</li><li>lower cross-entropy: model is better at predicting next word</li></ul><p><strong>Data compression:</strong></p><ul><li>If we designed an optimal code based on our LM model, we could encode the entire sentence in $H(P,\hat{P}) \times \text{number of words}$ bits. ($H(P,\hat{P})$ of Y/N questions for each word)</li></ul><p><strong>Perplexity:</strong></p><ul><li>LM performance is often reported as perplexity</li><li>$\text{perplexity} = 2^{\text{cross-entropy}}$</li></ul><h1 id="2-Smoothing-1"><a href="#2-Smoothing-1" class="headerlink" title="2. Smoothing 1"></a>2. Smoothing 1</h1><p>Smoothing methods address the problem by stealing probability mass from seen events and reallocating it to unseen events.</p><p>Otherwise, only using the N-gram model is useless since the assumption is too strict for natural language processing.</p><h2 id="2-1-Add-One-Laplace-Smoothing"><a href="#2-1-Add-One-Laplace-Smoothing" class="headerlink" title="2.1 Add-One (Laplace) Smoothing"></a>2.1 Add-One (Laplace) Smoothing</h2><p>$$<br>P_{+1}(w_i \mid w_{i-2},w_{i-1}) =\frac{C(w_{i-2},w_{i-1},w_i)+1}{C(w_{i-2},w_{i-1})+v} \\ \ \\ \text{where }\sum_{w_i \in V} \frac{C(w_{i-2},w_{i-1},w_i)+1}{C(w_{i-2},w_{i-1})+v}=1<br>$$</p><ul><li>$v = \text{vocabulary size}$</li><li>Cons: changed the denominator, which can have big eﬀects even on frequent events</li></ul><h2 id="2-2-Add-alpha-Lidstone-Smoothing"><a href="#2-2-Add-alpha-Lidstone-Smoothing" class="headerlink" title="2.2 Add-$\alpha$ (Lidstone) Smoothing"></a>2.2 Add-$\alpha$ (Lidstone) Smoothing</h2><p>$$<br>P_{+\alpha}(w_i \mid w_{i-2},w_{i-1}) =\frac{C(w_{i-2},w_{i-1},w_i)+\alpha}{C(w_{i-2},w_{i-1})+\alpha v}<br>$$</p><ul><li><p>Optimising $\alpha$ </p><img src="https://waylonli.com/img/mdimg/image-20200124104610419.png" alt="image-20200124104610419" style="zoom:46%;" /></li><li><p>Cons: </p><ul><li>changed the denominator, which can have big eﬀects even on frequent events</li><li>no good when vocabulary size is large</li></ul></li></ul><h2 id="2-3-Good-Turing"><a href="#2-3-Good-Turing" class="headerlink" title="2.3 Good-Turing"></a>2.3 Good-Turing</h2><p><strong>Idea:</strong></p><ul><li>change the numerator</li><li>use the frequency of singletons as a re-estimate of the frequency of zero-count bigrams</li></ul><p><strong>Adjusted counts:</strong><br>$$<br>c^* = (c+1)\frac{N_{c+1}}{N_c} \\ \ \\  P_{GT}=\frac{c^*}{n}<br>$$</p><p><strong>Assumption:</strong></p><ul><li><p>tye distribution of each bigram is binomail</p></li><li><p>each count is reduced slightly (Zipf): discounting</p><img src="https://waylonli.com/img/mdimg/image-20200126162032253.png" alt="image-20200126162032253" style="zoom:40%;" /></li></ul><p><strong>Cons:</strong></p><ul><li>assumes we know the vocabulary size</li><li>do not allow “holes” in the counts (some frequency has zero count)</li><li>applies discounts even to high-frequency items</li></ul><h1 id="3-Smoothing-2"><a href="#3-Smoothing-2" class="headerlink" title="3. Smoothing 2"></a>3. Smoothing 2</h1><p><strong>Problems of previous methods:</strong> assign equal probability to all unseen events</p><p><strong>Two ways:</strong></p><ul><li>interpolation</li><li>backoff</li></ul><h2 id="3-1-Interpolation"><a href="#3-1-Interpolation" class="headerlink" title="3.1 Interpolation"></a>3.1 Interpolation</h2><p><strong>Idea:</strong> combine higher and lower order N-gram models</p><ul><li>high-order N-grams are sensitive to more context, but have sparse counts</li><li>low-order N-grams have limited context, but robust counts</li></ul><p><strong>Model:</strong><br>$$<br>P_{INT}(w_3 \mid w_1,w_2) = \lambda_1P_1(w_3) + \lambda_2P_2(w_3 \mid w_2)+\lambda_3P_3(w_3\mid w_1,w_2)<br>$$</p><ul><li><p>$P_N$: N-gram estimate</p></li><li><p>$\lambda_i$: interpolation parameters / mixture weights</p><p>must sum to 1</p><img src="https://waylonli.com/img/mdimg/image-20200127211855299.png" alt="image-20200127211855299" style="zoom:38%;" /></li></ul><p><strong>Pick values of $\lambda_i$:</strong> machine learning method, optimise perplexity on a held-out data set</p><h2 id="3-2-Katz-Back-off"><a href="#3-2-Katz-Back-off" class="headerlink" title="3.2 Katz Back-off"></a>3.2 Katz Back-off</h2><p><strong>Idea:</strong> discount the trigram-based probability estimates</p><p><strong>Model:</strong></p><p>$$<br>T(n) = \begin{equation} \left\{ \begin{array}{lr} P^*(w_i \mid w_{i-N+1},\dots,w_{i-1}) \qquad \qquad  \ \text{if}\ \ \text{count}(w_{i-N+1},\dots,w_i)&gt;0,  \\ \alpha(w_{i-N+1},\dots,w_{i-1})P_{BO}(w_i \mid w_{i-N+2},\dots,w_{i-1}) \qquad \qquad  \text{else}.  \end{array} \right. \end{equation}<br>$$</p><ul><li>$ P^*(w_i \mid w_{i-N+1},\dots,w_{i-1})$: adjusted predictoin model (Good Turing)</li><li>$\alpha(w_1,w_{N-1})$: backoff weights</li><li>G-T if count is not 0, back off if count is 0</li></ul><h2 id="3-3-Kneser-Ney-smoothing"><a href="#3-3-Kneser-Ney-smoothing" class="headerlink" title="3.3 Kneser-Ney smoothing"></a>3.3 Kneser-Ney smoothing</h2><p><strong>Idea:</strong> takes diversity of histories into account</p><p><strong>Model:</strong><br>$$<br>N_{1+}(\bullet w_i)=\left| \{w_{i-1}:c(w_{i-1}, w_i)&gt;0\} \right |<br>$$</p><p>$$<br>P_{KN}(w_i)= \frac{N_{1+}(\bullet w_i)}{\sum_w N_{1+}(\bullet w)}<br>$$</p><table><thead><tr><th>Types</th><th>Methods</th></tr></thead><tbody><tr><td>Uniform probabilities</td><td>add-$\alpha$, Good-Turing</td></tr><tr><td>Probabilities from lower-order n-grams</td><td>interpolation, backoff</td></tr><tr><td>Probability of appering in new contexts</td><td>Kneser-Ney</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Smoothing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3-N-gram</title>
    <link href="/2020/01/21/3-N-gram/"/>
    <url>/2020/01/21/3-N-gram/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to N-gram language model</p><a id="more"></a><h1 id="1-Application"><a href="#1-Application" class="headerlink" title="1 Application:"></a>1 Application:</h1><ul><li>spelling correction</li><li>automatic speech recognition</li><li>machine translatio</li></ul><h1 id="2-Estimate-the-probabilities-training"><a href="#2-Estimate-the-probabilities-training" class="headerlink" title="2 Estimate the probabilities: training"></a>2 Estimate the probabilities: training</h1><ul><li>training data: large corpus of general English text</li><li>MLE: maximum likelihood estimation<ul><li>MLE assigns 0 probability to anything hasn’t occurred</li></ul></li><li>we don’t know the true probabilities, we estimate instead</li><li>sparse data problem: not enough observations to estimate probabilities well simply by counting observed data (for sentences, many will occur rarely in the training data, smater way: N-gram)</li></ul><h1 id="3-N-gram-language-model-N-gram-LM"><a href="#3-N-gram-language-model-N-gram-LM" class="headerlink" title="3 N-gram language model (N-gram LM)"></a>3 N-gram language model (N-gram LM)</h1><h2 id="3-1-Markov-assumption-independence-assumption"><a href="#3-1-Markov-assumption-independence-assumption" class="headerlink" title="3.1 Markov assumption (independence assumption)"></a>3.1 Markov assumption (independence assumption)</h2><p>The probability of a word only depends on a ﬁxed number of previous words (history)</p><ul><li><p>not always a good assumption</p><img src="https://waylonli.com/img/mdimg/image-20200121104301500.png" alt="image-20200121104301500" style="zoom:42%;" /></li></ul><h2 id="3-2-Probaility-calculation"><a href="#3-2-Probaility-calculation" class="headerlink" title="3.2 Probaility calculation"></a>3.2 Probaility calculation</h2><p>To estimate $ P(\vec{w}) $, use chain rule:<br>$$<br>P(w_1,\dots,w_n) = \prod_{i=1}^nP(w_i \mid w_1,w_2,\dots,w_{i-1}) \\ \approx P(w_1)P(w_2\mid w_1)\prod_{i=3}^n P(w_i \mid w_{i-2}, w_{i-1})<br>$$<br>Problem: sentence boundaryet</p><h2 id="3-3-Beginning-end-of-sequence"><a href="#3-3-Beginning-end-of-sequence" class="headerlink" title="3.3 Beginning / end of sequence"></a>3.3 Beginning / end of sequence</h2><img src="https://waylonli.com/img/mdimg/image-20200121105220976.png" alt="image-20200121105220976" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>N-gram</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-Corpora</title>
    <link href="/2020/01/17/2-Corpora/"/>
    <url>/2020/01/17/2-Corpora/</url>
    
    <content type="html"><![CDATA[<p>What is a corpora?</p><a id="more"></a><h1 id="1-Corpora"><a href="#1-Corpora" class="headerlink" title="1. Corpora"></a>1. Corpora</h1><p><strong>Def:</strong> a large or complete collection of writings</p><ul><li>For NLP, we need corpora with <strong>linguistic annotations</strong></li><li>Markup formats: XML, JSON, CoNLL-style</li><li>e.g. <em>Brown, WSJ, ECI, BNC, Redwoods, Gigaword, AMI, Google Books N-grams, Flickr 8K, English Visual Genome</em></li></ul><p><strong>Why need corpora</strong></p><ul><li>manual rules or database (rule-based, symbolic, knowledge-driven)</li><li>learning: provide example input/output pairs for supervised learning</li></ul><h1 id="2-Sentiment-Analysis"><a href="#2-Sentiment-Analysis" class="headerlink" title="2. Sentiment Analysis"></a>2. Sentiment Analysis</h1><p><strong>Goal:</strong> predict the opinion expressed in a piece of text (e.g. + or -, positive or negative)</p><p><strong>Method:</strong> attach gold labels to the reviews (+ or -)</p><ul><li><p>can be derived automatically from the original data artifact (metadata such as star ratings)</p></li><li><p>can be added by a human annotator who reads the text</p><p>Issue: annotators consistent or not</p></li></ul><p><strong>Simple sentiment classification algorithm</strong></p><ul><li>Use a sentiment lexicon to count positive and negative words</li><li>Issues<ul><li>sense ambiguity: need more data -&gt; use frequency counts to ascertain</li><li>sarcasm</li><li>text could mention expectation (expectation, not comment)</li><li>words may describe a character’s attitude, not the author’s</li><li>phrases like “can’t believe how great it is”</li></ul></li></ul><p><strong>Choice of training evalution data</strong></p><ul><li>Assume that the training data and the test data are sampled from the same distribution</li><li>Method: domain adaptation techniques</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Corpora</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-Intro</title>
    <link href="/2020/01/14/1-Intro/"/>
    <url>/2020/01/14/1-Intro/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to NLP</p><a id="more"></a><h1 id="1-Why-hard"><a href="#1-Why-hard" class="headerlink" title="1. Why hard?"></a>1. Why hard?</h1><ul><li><p>Ambiguity</p></li><li><p>Sparse data (due to <strong>Zipf’s Law</strong>)</p><p>not enough observations to estimate probabilities well simply by counting observed data.</p><p>$$f\times r \approx k$$</p><p>𝑓 = frequency of a word</p><p>𝑟 = rank of a word</p><p>𝑘 = a constant</p></li><li><p>Variation</p></li><li><p>Expressivity: one form can have different meanings and the same meaning can be expressed with different forms</p></li><li><p>Context dependence: correct interpretation is context-dependent and often requires world knowledge</p></li><li><p>Unknown representation: we don’t know how to represent the knowledge a human has / needs</p></li></ul><h1 id="2-Models"><a href="#2-Models" class="headerlink" title="2. Models"></a>2. Models</h1><ul><li><p>Non-probabilistic methods: FSMs, CKY parsers</p></li><li><p>Probabilistic models: HMMs for POS tagging, PCFGs for syntax</p><p>Probabilistic algorithms: Viterbi, probabilistic CKY</p></li></ul><h1 id="3-Zipf’s-Law"><a href="#3-Zipf’s-Law" class="headerlink" title="3. Zipf’s Law"></a>3. Zipf’s Law</h1><img src = "https://waylonranee.com/img/mdimg/image-20200508003917288.png" style="zoom:50%;"/><p><strong>Implications:</strong></p><ul><li>Regardless of how large the corpus is, there will be a lot of infrequent words</li><li>Holds for many other levels of linguistic structure (e.g. syntactic rules in a CFG)</li><li>Zipf’s Law shows that we need to find ways to estimate probabilities for things we have rarely or never seen during training</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
