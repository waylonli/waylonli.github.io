<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Real-Time Speech Recognition Using Python</title>
    <link href="/2020/03/05/Real-Time-Speech-Recognition-Using-Python/"/>
    <url>/2020/03/05/Real-Time-Speech-Recognition-Using-Python/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Environment"><a href="#1-Environment" class="headerlink" title="1. Environment"></a>1. Environment</h1><h2 id="1-1-Pyaudio"><a href="#1-1-Pyaudio" class="headerlink" title="1.1 Pyaudio"></a>1.1 Pyaudio</h2><ul><li>Windows: <code>pip install pyaudio</code></li><li>Linux: <code>sudo apt-get install python-pyaudio python3-pyaudio</code> </li><li>Mac OSX: <code>brew install portaudio</code> (need to <a href="https://osxdaily.com/2018/03/07/how-install-homebrew-mac-os/" target="_blank" rel="noopener">install Homebrew</a> on your mac first); then <code>pip install pyaudio</code></li></ul><h2 id="1-2-SpeechRecognition-package"><a href="#1-2-SpeechRecognition-package" class="headerlink" title="1.2 SpeechRecognition package"></a>1.2 SpeechRecognition package</h2><p><code>pip install SpeechRecognition</code></p><h1 id="2-Example-code"><a href="#2-Example-code" class="headerlink" title="2. Example code"></a>2. Example code</h1><p>Here is an example for recognising ‘yes’ and ‘no’:</p><pre><code class="python">import speech_recognition as srimport os# obtain audio from the microphoner = sr.Recognizer()t = Truewhile t:    with sr.Microphone() as source:        r.adjust_for_ambient_noise(source, duration = 0.5)          print(&#39;say something&#39;)        audio = r.record(source, duration = 2)    # listen for 2 seconds    output = r.recognize_google(audio, show_all = True)    if (len(output) &lt; 1):        print(&quot;Say louder&quot;)    # if the recogniser did not recognise anything from the microphone, ask speaker to say louder    else:        possible = [word[&#39;transcript&#39;] for word in output[&#39;alternative&#39;]]    # extract all the possible phrase from return dictionary        if (&quot;yes&quot; in possible):            print(&quot;yes&quot;)        if (&quot;no&quot; in possible):            print(&quot;no&quot;)        else:            print(&quot;Say it again&quot;)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Tutorials</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Speech-Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>8,9-POS tagging and HMMs</title>
    <link href="/2020/02/11/8,9-POS-tagging-and-HMMs/"/>
    <url>/2020/02/11/8,9-POS-tagging-and-HMMs/</url>
    
    <content type="html"><![CDATA[<p>Use Hidden Markov Models to do POS tagging</p><a id="more"></a><p><strong>Notation:</strong></p><ul><li><p>Sequence of observation overtime (sentence): $ O=o_1\dots o_T $ </p></li><li><p>Sequence of states (tags): $Q=q^1\dots q^N$</p><p>Sequence states over time: $Q = q_1 \dots q_T$ </p></li><li><p>Vocabulary size: $V$ </p></li><li><p>Time step: $t$, not a tag</p></li><li><p>Matrix of transition probabilities: $A$ </p><ul><li>$a_{ij}$: the prob of transitioning from state $q^i$ to $q^j$ </li></ul></li><li><p>Matrix of output probabilities: $B$ </p><ul><li>$b_i(o_t)$: the prob of emitting $o_t$ from state $q^i$ </li></ul></li></ul><h1 id="1-Pos-tagging"><a href="#1-Pos-tagging" class="headerlink" title="1. Pos tagging"></a>1. Pos tagging</h1><h2 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h2><p><strong>Uses:</strong></p><ul><li>First step towards syntactic analysis</li><li>Often useful for semantic analysis</li><li>Illustrate the use of HMMs</li></ul><p><strong>Depends on:</strong></p><ul><li>The word to be labeled</li><li>The labels of surrounding words</li></ul><p><strong>Tags:</strong> Penn Treebank POS tags</p><img src="https://waylonranee.com/img/mdimg/image-20200205210126948.png" srcset="/img/loading.gif" alt="image-20200205210126948" style="zoom:43%;" /><p><strong>Difficulties:</strong></p><ul><li>Ambiguity</li><li>Sparse data</li></ul><h2 id="1-2-Probabilistic-model-for-tagging-forward-algorithm"><a href="#1-2-Probabilistic-model-for-tagging-forward-algorithm" class="headerlink" title="1.2 Probabilistic model for tagging (forward algorithm?)"></a>1.2 Probabilistic model for tagging (forward algorithm?)</h2><p><strong>Assumption:</strong></p><ul><li>Each tag depends only on previous tag (bigram tag model)</li><li>Words are independent given tags</li></ul><p><strong>Finite-state machine:</strong> sentences are generated by walking through states in a graph, each state represents a tag</p><p><strong>Given:</strong></p><ul><li>The transition and output proabilities</li></ul><p>$$<br>P(O,Q) = \prod_{t=1}^n P(o_t \mid q_t) P(q_t \mid q_{t-1})<br>$$</p><h1 id="2-HMM"><a href="#2-HMM" class="headerlink" title="2. HMM"></a>2. HMM</h1><h2 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h2><p><strong>Purpose:</strong> find the best tag sequence for an untagged sentence</p><ul><li>Markov: Markov independence assumption (each tag / state only depends on fixed number of previous tags / states)</li><li>Hidden: at test time we only see the words / emissions, the tags / states are hidden variables</li></ul><p><strong>Elements:</strong></p><ul><li>a set of states (e.g. tags)</li><li>a set of output symbol (e.g. words)</li><li>initial state (e.g. beginning of the sentence</li><li>state transition probabilities (e.g. $P(t_i \mid t_{i-1})$)</li><li>symbol emission probabilities (e.g. $P(w_i \mid t_i)$)</li></ul><h2 id="2-2-Model"><a href="#2-2-Model" class="headerlink" title="2.2 Model"></a>2.2 Model</h2><p>$$<br>\underset{Q}{\operatorname{argmax}} P(Q \mid O) = \underset{Q}{\operatorname{argmax}} P(O \mid Q) P(Q)<br>$$</p><ul><li>Using Bayes’ rule</li><li>$P(Q) = \prod_t P(q_t \mid q_{t-1})$</li><li>$P(O \mid Q) = \prod_t P(o_t \mid q_t)$ </li></ul><p><strong>Joint probability:</strong><br>$$<br>P(O,Q\mid \lambda) = \prod_{t=1}^T P(o_t \mid q_t) P(q_{t-1}) = \prod_{t=1}^T b_{t}(o_t)a_{t-1,t}<br>$$</p><ul><li>$\lambda = (A,B)$ </li></ul><h2 id="2-3-Transition-and-Output-Probabilities-Matrix-A-B"><a href="#2-3-Transition-and-Output-Probabilities-Matrix-A-B" class="headerlink" title="2.3 Transition and Output Probabilities (Matrix $A,B$)"></a>2.3 Transition and Output Probabilities (Matrix $A,B$)</h2><ul><li>Transition matrix $A$: $a_{ij} = P(q_j \mid q_i)$ </li><li>Output matrix $B$: $b_i(o)=P(o \mid q^i)$ for output $o$ </li></ul><h2 id="2-4-Searching-Viterbi-algorithm"><a href="#2-4-Searching-Viterbi-algorithm" class="headerlink" title="2.4 Searching: Viterbi algorithm"></a>2.4 Searching: Viterbi algorithm</h2><p><strong>Decoding:</strong> finding the best tag sequence for a sentence is called decoding</p><p><strong>Intuition:</strong> the best path of length $t$ ending in state $Q$ must include the best path of length $t-1$ to the previous state</p><p><strong>Find $\underset{Q}{\operatorname{argmax}} P(Q \mid O) $:</strong><br>$$<br>v(j,t) = \max_{i=1}^N v(i,t-1) \cdot a_{ij} \cdot b_j<br>$$</p><ul><li><strong>Can also use negative log probabilities and take minimum over sum of costs</strong> </li></ul><p><strong>Chart:</strong> $N \times T$ </p><img src="https://waylonranee.com/img/mdimg/image-20200210220921409.png" srcset="/img/loading.gif" alt="image-20200210220921409" style="zoom:35%;" /><h2 id="2-5-Training-Baum-Welch-Forward-Backward-algorithm"><a href="#2-5-Training-Baum-Welch-Forward-Backward-algorithm" class="headerlink" title="2.5 Training: Baum-Welch (Forward-Backward) algorithm"></a>2.5 Training: Baum-Welch (Forward-Backward) algorithm</h2><p><strong>Compute the best parameters from unannotated corpus</strong></p><h3 id="2-5-1-Expectation-maximisation"><a href="#2-5-1-Expectation-maximisation" class="headerlink" title="2.5.1 Expectation-maximisation"></a>2.5.1 Expectation-maximisation</h3><ul><li>Initialise parameters $\lambda^{(0)}$ </li><li>At each iteration $k$,<ul><li>E-step: compute expected counts using $\lambda^{(k-1)}$ </li><li>M-step: set $\lambda^{(k)}$ using MLE on the expected counts</li></ul></li><li>Repeat until $\lambda$ does not change</li></ul><p>PS:</p><ul><li>Can find a local maximum, not guarantee to find the global maximum</li></ul><h3 id="2-5-2-Baum-Welch"><a href="#2-5-2-Baum-Welch" class="headerlink" title="2.5.2 Baum-Welch"></a>2.5.2 Baum-Welch</h3><p><strong>Idea:</strong> computes expected counts using forward probabilities and backward probabilities<br>$$<br>\beta(j,t) = P(q^t = j, o_{t+1}, o_{t+2}, \dots,o_T \mid \lambda)<br>$$<br>PS: EM is much more general, can use for many latent variable models</p><h2 id="2-6-Evaluation-Forward-algorithm-sum-instead-of-max-compute-the-likelihood"><a href="#2-6-Evaluation-Forward-algorithm-sum-instead-of-max-compute-the-likelihood" class="headerlink" title="2.6 Evaluation: Forward algorithm (sum instead of max): compute the likelihood"></a>2.6 Evaluation: Forward algorithm (sum instead of max): compute the likelihood</h2><p>$$<br>a(j,t) = \sum_{i=1}^N a(i,t-1)\cdot a_{ij}\cdot b_j(o_t)<br>$$</p><ul><li>Actually a language model, not just count for the one previous step, but also all the earlier steps</li></ul><h2 id="2-7-Three-main-problems"><a href="#2-7-Three-main-problems" class="headerlink" title="2.7 Three main problems"></a>2.7 Three main problems</h2><ol><li><strong>Evaluation problem</strong><ul><li>Evaluation problem answers the question: what is the probability that a particular sequence of symbols is produced by a particular model?</li><li>For evaluation we use two algorithms: the <em>forward algorithm</em> or the <em>backwards algorithm</em> (DO NOT confuse them with the forward-backward algorithm).</li></ul></li><li><strong>Decoding problem</strong><ul><li>Decoding problem answers the question: Given a sequence of symbols (your observations) and a model, what is the most likely sequence of states that produced the sequence.</li><li>For decoding we use the <em>Viterbi algorithm</em>.</li></ul></li><li><strong>Training problem</strong><ul><li>Training problem answers the question: Given a model structure and a set of sequences, find the model that best fits the data.</li><li>For this problem we can use the following 3 algorithms:<ol><li>MLE (maximum likelihood estimation)</li><li>Viterbi training(DO NOT confuse with Viterbi decoding)</li><li>Baum Welch = forward-backward algorithm</li></ol></li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>POS-Tagging</tag>
      
      <tag>HMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Structural Testing</title>
    <link href="/2020/02/05/Structural-Testing/"/>
    <url>/2020/02/05/Structural-Testing/</url>
    
    <content type="html"><![CDATA[<p><strong>Def:</strong> judging test suite thoroughness based on the structure of the program itself</p><ul><li>Also known as <strong>white-box testing</strong></li><li>Distinguish from functional testing (black-box testing)<ul><li>structural testing is still testing product functionality against its specification</li><li>only the measure of thoroughness has changed</li></ul></li></ul><p><strong>Why need structural testing:</strong></p><ul><li>Executing all control flow elements does not guarantee finding all faults</li><li>Increase confidence in thoroughness of testing</li></ul><p><strong>Practical use:</strong></p><ul><li>Firstly create functional test suite</li><li>Then measure structural coverage to identify see what is missing</li></ul><h2 id="Statement-testing-amp-Branch-testing"><a href="#Statement-testing-amp-Branch-testing" class="headerlink" title="Statement testing &amp; Branch testing"></a>Statement testing &amp; Branch testing</h2><p><strong>Statement testing:</strong></p><ul><li>Adequacy criterion: each statement must be executed at least once</li><li>Rationale: a fault in a statement can only be revealed by executing the faulty statement</li></ul><p><strong>Branch testing:</strong></p><ul><li>Adequacy criterion: each branch (edge in the CFG) must be executed at least once</li></ul><p><strong>Comparison:</strong></p><p>Traversing all edges of a graph causes all nodes to be visited, so we have:</p><ul><li>Test suites that satisfy the branch adequacy criterion for a program P also satisfy the statement adequacy criterion for the same program</li><li>A statement-adequate (or node-adequate) test suite may not be branch-adequate (edge-adequate)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Software-Testing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Software-Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>7-Text Classification</title>
    <link href="/2020/02/04/7-Text-classification/"/>
    <url>/2020/02/04/7-Text-classification/</url>
    
    <content type="html"><![CDATA[<p>Introduce two different methods to do text classification: Naive Bayes and Maximum Entropy</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><strong>Application:</strong></p><ul><li>spam detection (binary: spam / not spam)</li><li>sentiment analysis (binary or multiway)<ul><li>review (pos / neg or 1-5 stars)</li><li>political argument (pro / con, or pro / con / neutral)</li></ul></li><li>topic classification (multiway: sport / finance / travel, etc.)</li><li>categorise the author<ul><li>native language identification</li><li>diagnosis of disease</li><li>gender, dialect, educational background, political, orientation</li></ul></li></ul><p><strong>Why not N-gram:</strong></p><ul><li>sequential relationships are largely irrelevant for many tasks (bag of words assumption)</li><li>want to include other features (e.g. POS tags) that N-gram models do not include</li></ul><p><strong>Two methods:</strong></p><ul><li>Naive Bayes</li><li>Maximum Entropy (multinomial logistic regression)</li></ul><h1 id="2-Naive-Bayes"><a href="#2-Naive-Bayes" class="headerlink" title="2. Naive Bayes"></a>2. Naive Bayes</h1><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p>$$<br>\hat{c} = \underset{c\in C}{\operatorname{argmax}} P(c\mid d) = \underset{c\in C}{\operatorname{argmax}} \frac{P(d \mid c)P(c)}{P(d)} = \underset{c\in C}{\operatorname{argmax}}P(d \mid c)P(c)<br>$$</p><ul><li><p>Naive Bayes assumption: $P(d \mid c) = P(f_1, f_2,\dots ,f_n \mid c) \approx P(f_1 \mid c)P(f_2\mid c)\dots P(f_n \mid c)$<br>$$<br>\ \\ \hat{P}(f_i \mid c) = \frac{\text{count}(f_i,c)+\alpha}{\sum_{f \in F}(\text{count}(f,c)+\alpha)}<br>$$<br>PS: $\alpha$ needs to be added in calculations for every class, so the total count should add $\left |F\right |*\alpha$ </p><ul><li>$F$: the set of possible features</li><li>$\alpha$: smoothing parameter</li><li>all features not shown earlier before have $\hat{P}(f_i \mid +) = \frac{\alpha}{68+\alpha \left |F\right |}$ </li></ul></li><li><p>$P(c)$: estimated with MLE<br>$$<br>\hat{P}(c) = \frac{N_c}{N}<br>$$</p></li></ul><h2 id="2-2-Step"><a href="#2-2-Step" class="headerlink" title="2.2 Step"></a>2.2 Step</h2><ul><li><p>Test document $d$ </p></li><li><p>Calculate<br>$$<br>P(+ \mid d) \propto P(+) \prod_{i=1}^n P(f_i \mid +) \\ \  \\ P(-\mid d) \propto P(-) \prod_{i=1}^n P(f_i \mid -)<br>$$</p></li><li><p>Choose the one with larger value</p></li></ul><h2 id="2-3-Choose-features"><a href="#2-3-Choose-features" class="headerlink" title="2.3 Choose features"></a>2.3 Choose features</h2><ul><li>Binary value for $f_i$</li><li>Use subset of $\left | F \right |$ (e.g. ignore stop words)</li><li>Use more complex features (e.g. bigrams, etc.)</li></ul><h2 id="2-4-Deal-with-very-small-numbers-underflow-use-costs"><a href="#2-4-Deal-with-very-small-numbers-underflow-use-costs" class="headerlink" title="2.4 Deal with very small numbers (underflow): use costs"></a>2.4 Deal with very small numbers (underflow): use costs</h2><p>Find the lowest cost classification<br>$$<br>\hat{c} = \underset{c\in C}{\operatorname{argmin}}  (-\log P(c) + \sum_{i=1}^n - \log P(f_i \mid c))<br>$$</p><ul><li>Costs are negative log probabilities</li><li>Can avoid underflow by turning multiplication to sum</li><li>Look for the lowest cost overall</li></ul><h2 id="2-5-Implementation"><a href="#2-5-Implementation" class="headerlink" title="2.5 Implementation"></a>2.5 Implementation</h2><ul><li>Naive Bayes is a <strong>linear classifier</strong> since it uses a  linear function of the input feature</li><li>As is logistic regression</li></ul><h2 id="2-6-Problems-Pros-amp-Cons"><a href="#2-6-Problems-Pros-amp-Cons" class="headerlink" title="2.6 Problems, Pros &amp; Cons"></a>2.6 Problems, Pros &amp; Cons</h2><p><strong>Problem:</strong></p><ul><li><p>might need domain-specific non-sentiment words</p><p>e.g. memory, CPU for computer product reviews</p></li><li><p>stopwords might be very useful features for some tasks</p></li><li><p>probably better to use too many irrelevant features than not enough relevant features</p></li></ul><p><strong>Pros:</strong></p><ul><li>Easy to implement</li><li>Fast to train</li><li>Do not need much training data compared with other methods (good for small datasets)</li><li>Usually works reasonably well</li><li>Can be a baseline method for any classification task</li></ul><p><strong>Cons:</strong></p><ul><li>Assumption is naive!<ul><li>features are not usually independent given the class</li><li>adding feature types often leads to even stronger correlations between features</li><li>accuracy can somtimes be OK, but will be highly <strong>overconfident</strong> in its decisions</li></ul></li></ul><h1 id="3-Maximum-Entropy-Model"><a href="#3-Maximum-Entropy-Model" class="headerlink" title="3. Maximum Entropy Model"></a>3. Maximum Entropy Model</h1><h2 id="3-1-Model"><a href="#3-1-Model" class="headerlink" title="3.1 Model"></a>3.1 Model</h2><p>$$<br>P(c \mid \vec{x}) = \frac{1}{Z}\exp(\sum_i w_i f_i(\vec{x},c)) \\ \ \\ \text{where } Z = \sum_{c’} \exp(\sum_iw_i f_i(\vec{x},c))<br>$$</p><ul><li><strong>Multinomial logistic regression</strong>, model $P(c \mid d)$ directly instead of using Bayes’ rule</li><li>Use vector to represent the observed data</li><li>End up choosing the class for whose $\vec{w} \cdot \vec{f}$ is the highest</li></ul><p>PS:</p><ul><li>For each class, $\vec{w}$ varies since each word cause different influence on the class</li></ul><h2 id="3-2-Training"><a href="#3-2-Training" class="headerlink" title="3.2 Training"></a>3.2 Training</h2><p><strong>Conditional maximum likelihood estimation (CMLE):</strong></p><p>Given instances $x^{(1)} \dots x^{(N)}$ with labels $c^{(1)}\dots c^{(N)}$<br>$$<br>\vec{w} = \underset{\vec{w}}{\operatorname{argmax}} \sum_j \log P(c^{(j)}\mid x^{(j)})<br>$$<br><strong>Avoid overfitting:</strong> regularisation</p><h2 id="3-3-Pros-amp-Cons"><a href="#3-3-Pros-amp-Cons" class="headerlink" title="3.3 Pros &amp; Cons"></a>3.3 Pros &amp; Cons</h2><p><strong>Pros:</strong></p><ul><li>Better performance</li><li>Do not need conditionally independent assumption</li></ul><p><strong>Cons:</strong></p><ul><li><strong>Time-consuming</strong> if there are a large number of classes and / or thousands of features to extract from each training example</li></ul><h2 id="3-4-Why-is-called-MaxEnt"><a href="#3-4-Why-is-called-MaxEnt" class="headerlink" title="3.4 Why is called MaxEnt?"></a>3.4 Why is called MaxEnt?</h2><ul><li>The probabilistic model we are building should follow whatever constraints we impose on it</li><li>But beyond these constraints we should follow Occam’s Razor, make the fewest possible assumptions</li></ul><h1 id="4-Summary"><a href="#4-Summary" class="headerlink" title="4. Summary"></a>4. Summary</h1><h2 id="4-1-Differences"><a href="#4-1-Differences" class="headerlink" title="4.1 Differences"></a>4.1 Differences</h2><table><thead><tr><th align="center"></th><th align="center">Naive Bayes</th><th align="center">MaxEnt</th></tr></thead><tbody><tr><td align="center">Model</td><td align="center">MLE (maximum likelihood estimation)<br />Bayes’ rule</td><td align="center">CMLE (conditional maximum likelihood estimation)<br />Multinomial logistic regression</td></tr><tr><td align="center">Features</td><td align="center">Directly observed</td><td align="center">Use vector representation<br />Features are functions that depend on both observation $\vec{x}$ and class $c$</td></tr><tr><td align="center">Assumption</td><td align="center">Conditionally independent assumption</td><td align="center">None</td></tr></tbody></table><h2 id="4-2-Similarity"><a href="#4-2-Similarity" class="headerlink" title="4.2 Similarity"></a>4.2 Similarity</h2><ul><li>Both are easily available in standard ML toolkits</li><li>Both need to choose what features are good to use </li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Naive-Bayes</tag>
      
      <tag>Maximum-Entropy-Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>6-Spelling correction, edit distance and EM</title>
    <link href="/2020/01/30/6-Spelling-correction-edit-distance-and-EM/"/>
    <url>/2020/01/30/6-Spelling-correction-edit-distance-and-EM/</url>
    
    <content type="html"><![CDATA[<p>Introduction to how to do spelling correction</p><a id="more"></a><h1 id="1-Spelling-correction"><a href="#1-Spelling-correction" class="headerlink" title="1 Spelling correction"></a>1 Spelling correction</h1><p>##1.1 Simple version</p><p><strong>Assumption:</strong></p><ul><li>have a large dictionary of real word</li><li>only correct non-word</li><li>only consider corrections differ by a <strong>single</strong> character from the non-word</li></ul><p><strong>Idea:</strong></p><ul><li>generate a list of all words $y$ differ by 1 character from $x$</li><li>compute the probability of each $y$ and choose the highest one</li></ul><h2 id="1-2-Alignments-and-edit-distance"><a href="#1-2-Alignments-and-edit-distance" class="headerlink" title="1.2 Alignments and edit distance"></a>1.2 Alignments and edit distance</h2><p><strong>Edit distance:</strong></p><ul><li>substitution</li><li>deletion</li><li>insertion</li></ul><p><strong>Need to:</strong></p><ul><li><p>find the optimal character alignment between two words</p><p>the optimal alignment is the one with <strong>minimum edit distance (MED)</strong></p></li></ul><p><strong>How:</strong></p><ul><li>Dynamic programming: Viterbi, CKY</li></ul><p><strong>Uses:</strong></p><ul><li>spelling correction</li><li>morphological analysis</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Edit-distance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5-Distributed representations</title>
    <link href="/2020/01/28/5-Distributed-representations/"/>
    <url>/2020/01/28/5-Distributed-representations/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to word embedding</p><a id="more"></a><h1 id="1-Distributed-word-representation-Word2Vec"><a href="#1-Distributed-word-representation-Word2Vec" class="headerlink" title="1 Distributed word representation (Word2Vec)"></a>1 Distributed word representation (Word2Vec)</h1><p>Can represent the similarity of two words</p><p><strong>Early:</strong> </p><ul><li>define classes $c$ of words, by hand or automatically</li><li>$P_{CL}(w_i \mid w_{i-1})=P(c_i \mid c_{i-1})P(w_i \mid c_i)$</li></ul><p><strong>Recent idea:</strong> use neural networks to project words into a continuous space, so words that appear in similar contexts have similar representation (usually 50 or 100 dimensions)</p><p><strong>Pros:</strong></p><ul><li>can represent the similarity (distance) between different words</li></ul><h2 id="1-1-How-to-train"><a href="#1-1-How-to-train" class="headerlink" title="1.1 How to train?"></a>1.1 How to train?</h2><p>No direct way to train word vectors, but we can get word vectors while training our language model</p><p><strong>Models for one-hot representation:</strong> neural network language model (NNLM), recurrent neural network language model (RNNLM)</p><ul><li>calculations are too complicated</li><li>too many parameters</li></ul><p><strong>Models for Word2Vec:</strong> </p><ul><li>Continuous Bag of Words (CBOW): predict central word based on the context</li><li>Skip-gram: predict context based on the central word</li></ul><h3 id="1-1-1-CBOW"><a href="#1-1-1-CBOW" class="headerlink" title="1.1.1 CBOW"></a>1.1.1 CBOW</h3><img src = "https://waylonranee.com/img/mdimg/image-20200128213558577.png" style="zoom:50%;"/><p><strong>Loss function:</strong><br>$$<br>L=\sum_{w \in C} \log P(w \mid \text{Context}(w))<br>$$</p><ul><li>no hidden layer (just one projection layer)</li><li>two directions window</li><li>input: one-hot representations</li><li>simplify projection layer as a sum and a mean operation</li></ul><p>After getting the weights matrix $w$, multiply one-hot representation and $w$, we can get the distributed representation of that word</p><ul><li>accelerating method: Hierarchical softmax, Negative sampling</li></ul><h3 id="1-1-2-Skip-gram"><a href="#1-1-2-Skip-gram" class="headerlink" title="1.1.2 Skip-gram"></a>1.1.2 Skip-gram</h3><img src = "https://waylonranee.com/img/mdimg/image-20200128214415619.png" style="zoom:50%;"/><p><strong>Loss function:</strong><br>$$<br>L=\sum_{w \in C} \log P(\text{context}(w) \mid w)<br>$$</p><ul><li><p>two parameters</p><ul><li><p>skip_window: window size = 2*skip_window + 1</p></li><li><p>num_skips: pick num_skips pairs of words</p><p>e.g. I am a boy who loves football.</p><p>for num_skips = 2, skip_window = 2, word = a</p><p>=&gt; (a, I), (a, am), (a,boy), (a,who)</p></li></ul></li><li><p>input: one-hot representation of $w(t)$ </p></li></ul><p>##1.2 Problem</p><ul><li>the use of window size ignore the information contained in the whole text</li><li>Word vector cannot represent different meanings of a single word</li></ul><p>Solution: Glove</p><h1 id="2-Noisy-channel-model"><a href="#2-Noisy-channel-model" class="headerlink" title="2 Noisy channel model"></a>2 Noisy channel model</h1><img src = "https://waylonranee.com/img/mdimg/image-20200128104730602.png" style="zoom:50%;"/>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Word2Vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4-Evaluation and smoothing</title>
    <link href="/2020/01/23/4-Evaluation-and-smoothing/"/>
    <url>/2020/01/23/4-Evaluation-and-smoothing/</url>
    
    <content type="html"><![CDATA[<p>Introduction to smoothing word counts to get the probabilities</p><a id="more"></a><h1 id="1-Evaluation"><a href="#1-Evaluation" class="headerlink" title="1. Evaluation"></a>1. Evaluation</h1><ul><li>Extrinstic: measure performance on a downstream application<ul><li>plug different models into the system, test on the same dataset and compare the accuracy</li></ul></li><li>Intrinstic: design a measure that is inherent to the current task</li></ul><p><strong>Entropy:</strong> human need $H(X)$ of Y/N questions to determine the next word<br>$$<br>H(X)=\sum_x -P(x) \log_2P(x)<br>$$<br><strong>Cross-entropy:</strong><br>$$<br>H(P,\hat{P}) = \sum_x -P(x) \log_2 \hat{P}(x)<br>$$</p><ul><li>cross-entropy $\ge$ entropy</li><li>lower cross-entropy: model is better at predicting next word</li></ul><p><strong>Data compression:</strong></p><ul><li>If we designed an optimal code based on our LM model, we could encode the entire sentence in $H(P,\hat{P}) \times \text{number of words}$ bits. ($H(P,\hat{P})$ of Y/N questions for each word)</li></ul><p><strong>Perplexity:</strong></p><ul><li>LM performance is often reported as perplexity</li><li>$\text{perplexity} = 2^{\text{cross-entropy}}$</li></ul><h1 id="2-Smoothing-1"><a href="#2-Smoothing-1" class="headerlink" title="2. Smoothing 1"></a>2. Smoothing 1</h1><p>Smoothing methods address the problem by stealing probability mass from seen events and reallocating it to unseen events.</p><p>Otherwise, only using the N-gram model is useless since the assumption is too strict for natural language processing.</p><h2 id="2-1-Add-One-Laplace-Smoothing"><a href="#2-1-Add-One-Laplace-Smoothing" class="headerlink" title="2.1 Add-One (Laplace) Smoothing"></a>2.1 Add-One (Laplace) Smoothing</h2><p>$$<br>P_{+1}(w_i \mid w_{i-2},w_{i-1}) =\frac{C(w_{i-2},w_{i-1},w_i)+1}{C(w_{i-2},w_{i-1})+v} \\ \ \\ \text{where }\sum_{w_i \in V} \frac{C(w_{i-2},w_{i-1},w_i)+1}{C(w_{i-2},w_{i-1})+v}=1<br>$$</p><ul><li>$v = \text{vocabulary size}$</li><li>Cons: changed the denominator, which can have big eﬀects even on frequent events</li></ul><h2 id="2-2-Add-alpha-Lidstone-Smoothing"><a href="#2-2-Add-alpha-Lidstone-Smoothing" class="headerlink" title="2.2 Add-$\alpha$ (Lidstone) Smoothing"></a>2.2 Add-$\alpha$ (Lidstone) Smoothing</h2><p>$$<br>P_{+\alpha}(w_i \mid w_{i-2},w_{i-1}) =\frac{C(w_{i-2},w_{i-1},w_i)+\alpha}{C(w_{i-2},w_{i-1})+\alpha v}<br>$$</p><ul><li><p>Optimising $\alpha$ </p><img src="https://waylonranee.com/img/mdimg/image-20200124104610419.png" srcset="/img/loading.gif" alt="image-20200124104610419" style="zoom:46%;" /></li><li><p>Cons: </p><ul><li>changed the denominator, which can have big eﬀects even on frequent events</li><li>no good when vocabulary size is large</li></ul></li></ul><h2 id="2-3-Good-Turing"><a href="#2-3-Good-Turing" class="headerlink" title="2.3 Good-Turing"></a>2.3 Good-Turing</h2><p><strong>Idea:</strong></p><ul><li>change the numerator</li><li>use the frequency of singletons as a re-estimate of the frequency of zero-count bigrams</li></ul><p><strong>Adjusted counts:</strong><br>$$<br>c^* = (c+1)\frac{N_{c+1}}{N_c} \\ \ \\  P_{GT}=\frac{c^*}{n}<br>$$</p><p><strong>Assumption:</strong></p><ul><li><p>tye distribution of each bigram is binomail</p></li><li><p>each count is reduced slightly (Zipf): discounting</p><img src="https://waylonranee.com/img/mdimg/image-20200126162032253.png" srcset="/img/loading.gif" alt="image-20200126162032253" style="zoom:40%;" /></li></ul><p><strong>Cons:</strong></p><ul><li>assumes we know the vocabulary size</li><li>do not allow “holes” in the counts (some frequency has zero count)</li><li>applies discounts even to high-frequency items</li></ul><h1 id="3-Smoothing-2"><a href="#3-Smoothing-2" class="headerlink" title="3. Smoothing 2"></a>3. Smoothing 2</h1><p><strong>Problems of previous methods:</strong> assign equal probability to all unseen events</p><p><strong>Two ways:</strong></p><ul><li>interpolation</li><li>backoff</li></ul><h2 id="3-1-Interpolation"><a href="#3-1-Interpolation" class="headerlink" title="3.1 Interpolation"></a>3.1 Interpolation</h2><p><strong>Idea:</strong> combine higher and lower order N-gram models</p><ul><li>high-order N-grams are sensitive to more context, but have sparse counts</li><li>low-order N-grams have limited context, but robust counts</li></ul><p><strong>Model:</strong><br>$$<br>P_{INT}(w_3 \mid w_1,w_2) = \lambda_1P_1(w_3) + \lambda_2P_2(w_3 \mid w_2)+\lambda_3P_3(w_3\mid w_1,w_2)<br>$$</p><ul><li><p>$P_N$: N-gram estimate</p></li><li><p>$\lambda_i$: interpolation parameters / mixture weights</p><p>must sum to 1</p><img src="https://waylonranee.com/img/mdimg/image-20200127211855299.png" srcset="/img/loading.gif" alt="image-20200127211855299" style="zoom:38%;" /></li></ul><p><strong>Pick values of $\lambda_i$:</strong> machine learning method, optimise perplexity on a held-out data set</p><h2 id="3-2-Katz-Back-off"><a href="#3-2-Katz-Back-off" class="headerlink" title="3.2 Katz Back-off"></a>3.2 Katz Back-off</h2><p><strong>Idea:</strong> discount the trigram-based probability estimates</p><p><strong>Model:</strong></p><p>$$<br>T(n) = \begin{equation} \left\{ \begin{array}{lr} P^*(w_i \mid w_{i-N+1},\dots,w_{i-1}) \qquad \qquad  \ \text{if}\ \ \text{count}(w_{i-N+1},\dots,w_i)&gt;0,  \\ \alpha(w_{i-N+1},\dots,w_{i-1})P_{BO}(w_i \mid w_{i-N+2},\dots,w_{i-1}) \qquad \qquad  \text{else}.  \end{array} \right. \end{equation}<br>$$</p><ul><li>$ P^*(w_i \mid w_{i-N+1},\dots,w_{i-1})$: adjusted predictoin model (Good Turing)</li><li>$\alpha(w_1,w_{N-1})$: backoff weights</li><li>G-T if count is not 0, back off if count is 0</li></ul><h2 id="3-3-Kneser-Ney-smoothing"><a href="#3-3-Kneser-Ney-smoothing" class="headerlink" title="3.3 Kneser-Ney smoothing"></a>3.3 Kneser-Ney smoothing</h2><p><strong>Idea:</strong> takes diversity of histories into account</p><p><strong>Model:</strong><br>$$<br>N_{1+}(\bullet w_i)=\left| \{w_{i-1}:c(w_{i-1}, w_i)&gt;0\} \right |<br>$$</p><p>$$<br>P_{KN}(w_i)= \frac{N_{1+}(\bullet w_i)}{\sum_w N_{1+}(\bullet w)}<br>$$</p><table><thead><tr><th>Types</th><th>Methods</th></tr></thead><tbody><tr><td>Uniform probabilities</td><td>add-$\alpha$, Good-Turing</td></tr><tr><td>Probabilities from lower-order n-grams</td><td>interpolation, backoff</td></tr><tr><td>Probability of appering in new contexts</td><td>Kneser-Ney</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Smoothing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3-N-gram</title>
    <link href="/2020/01/21/3-N-gram/"/>
    <url>/2020/01/21/3-N-gram/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to N-gram language model</p><a id="more"></a><h1 id="1-Application"><a href="#1-Application" class="headerlink" title="1 Application:"></a>1 Application:</h1><ul><li>spelling correction</li><li>automatic speech recognition</li><li>machine translatio</li></ul><h1 id="2-Estimate-the-probabilities-training"><a href="#2-Estimate-the-probabilities-training" class="headerlink" title="2 Estimate the probabilities: training"></a>2 Estimate the probabilities: training</h1><ul><li>training data: large corpus of general English text</li><li>MLE: maximum likelihood estimation<ul><li>MLE assigns 0 probability to anything hasn’t occurred</li></ul></li><li>we don’t know the true probabilities, we estimate instead</li><li>sparse data problem: not enough observations to estimate probabilities well simply by counting observed data (for sentences, many will occur rarely in the training data, smater way: N-gram)</li></ul><h1 id="3-N-gram-language-model-N-gram-LM"><a href="#3-N-gram-language-model-N-gram-LM" class="headerlink" title="3 N-gram language model (N-gram LM)"></a>3 N-gram language model (N-gram LM)</h1><h2 id="3-1-Markov-assumption-independence-assumption"><a href="#3-1-Markov-assumption-independence-assumption" class="headerlink" title="3.1 Markov assumption (independence assumption)"></a>3.1 Markov assumption (independence assumption)</h2><p>The probability of a word only depends on a ﬁxed number of previous words (history)</p><ul><li><p>not always a good assumption</p><img src="https://waylonranee.com/img/mdimg/image-20200121104301500.png" srcset="/img/loading.gif" alt="image-20200121104301500" style="zoom:42%;" /></li></ul><h2 id="3-2-Probaility-calculation"><a href="#3-2-Probaility-calculation" class="headerlink" title="3.2 Probaility calculation"></a>3.2 Probaility calculation</h2><p>To estimate $ P(\vec{w}) $, use chain rule:<br>$$<br>P(w_1,\dots,w_n) = \prod_{i=1}^nP(w_i \mid w_1,w_2,\dots,w_{i-1}) \\ \approx P(w_1)P(w_2\mid w_1)\prod_{i=3}^n P(w_i \mid w_{i-2}, w_{i-1})<br>$$<br>Problem: sentence boundaryet</p><h2 id="3-3-Beginning-end-of-sequence"><a href="#3-3-Beginning-end-of-sequence" class="headerlink" title="3.3 Beginning / end of sequence"></a>3.3 Beginning / end of sequence</h2><img src="https://waylonranee.com/img/mdimg/image-20200121105220976.png" srcset="/img/loading.gif" alt="image-20200121105220976" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>N-gram</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-Corpora</title>
    <link href="/2020/01/17/2-Corpora/"/>
    <url>/2020/01/17/2-Corpora/</url>
    
    <content type="html"><![CDATA[<p>What is a corpora?</p><a id="more"></a><h1 id="1-Corpora"><a href="#1-Corpora" class="headerlink" title="1. Corpora"></a>1. Corpora</h1><p><strong>Def:</strong> a large or complete collection of writings</p><ul><li>For NLP, we need corpora with <strong>linguistic annotations</strong></li><li>Markup formats: XML, JSON, CoNLL-style</li><li>e.g. <em>Brown, WSJ, ECI, BNC, Redwoods, Gigaword, AMI, Google Books N-grams, Flickr 8K, English Visual Genome</em></li></ul><p><strong>Why need corpora</strong></p><ul><li>manual rules or database (rule-based, symbolic, knowledge-driven)</li><li>learning: provide example input/output pairs for supervised learning</li></ul><h1 id="2-Sentiment-Analysis"><a href="#2-Sentiment-Analysis" class="headerlink" title="2. Sentiment Analysis"></a>2. Sentiment Analysis</h1><p><strong>Goal:</strong> predict the opinion expressed in a piece of text (e.g. + or -, positive or negative)</p><p><strong>Method:</strong> attach gold labels to the reviews (+ or -)</p><ul><li><p>can be derived automatically from the original data artifact (metadata such as star ratings)</p></li><li><p>can be added by a human annotator who reads the text</p><p>Issue: annotators consistent or not</p></li></ul><p><strong>Simple sentiment classification algorithm</strong></p><ul><li>Use a sentiment lexicon to count positive and negative words</li><li>Issues<ul><li>sense ambiguity: need more data -&gt; use frequency counts to ascertain</li><li>sarcasm</li><li>text could mention expectation (expectation, not comment)</li><li>words may describe a character’s attitude, not the author’s</li><li>phrases like “can’t believe how great it is”</li></ul></li></ul><p><strong>Choice of training evalution data</strong></p><ul><li>Assume that the training data and the test data are sampled from the same distribution</li><li>Method: domain adaptation techniques</li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Corpora</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-Intro</title>
    <link href="/2020/01/14/1-Intro/"/>
    <url>/2020/01/14/1-Intro/</url>
    
    <content type="html"><![CDATA[<p>Brief introduction to NLP</p><a id="more"></a><h1 id="Why-hard"><a href="#Why-hard" class="headerlink" title="Why hard?"></a>Why hard?</h1><ul><li><p>Ambiguity</p></li><li><p>Sparse data (due to <strong>Zipf’s Law</strong>)</p><p>$$f\times r \approx k$$</p><p>𝑓 = frequency of a word</p><p>𝑟 = rank of a word</p><p>𝑘 = a constant</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Natural-Language-Processing</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
